# Agentic Sequential Recommendation System

A comprehensive multi-agent recommendation system combining **MACRec's agent architecture**, **LLM-Sequential-Recommendation's dataset processing**, and **ThinkRec's dual-objective training** approach.

## üöÄ Quick Start


```bash

source venv/bin/activate
# Install dependencies
pip install -r requirements.txt

# Run complete system demo
python examples/complete_pipeline_demo.py

# Run specific tests
python tests/test_agents.py
python tests/test_integration.py
```

## üèóÔ∏è System Architecture

### Core Components

1. **Multi-Agent System** (MACRec-inspired)
   - **Manager**: Central orchestrator with think/act mechanism
   - **Analyst**: User and item sequential pattern analysis  
   - **Reflector**: Iterative improvement through reflection

2. **Dataset Processing** (LLM-Sequential-Recommendation compatible)
   - Beauty dataset (Amazon 5-core)
   - Delivery Hero dataset
   - Leave-one-out evaluation setup

3. **Evaluation Framework**
   - Hit Rate@K, NDCG@K, MRR metrics
   - Comprehensive performance tracking
   - Reflection-based improvements

### Agent Communication Protocol

```python
# Example workflow
from agentic_recommender.system import create_pipeline
from agentic_recommender.core import RecommendationRequest
from agentic_recommender.models.llm_provider import GeminiProvider

# Initialize system
llm = GeminiProvider(api_key="your-api-key")
pipeline = create_pipeline(llm, "beauty", "reflexion")

# Load dataset
pipeline.load_dataset("path/to/data.json", use_synthetic=True)

# Make recommendation
request = RecommendationRequest(
    user_id="user_123",
    user_sequence=["laptop", "mouse", "keyboard"],
    candidates=["monitor", "headphones", "webcam"]
)

response = pipeline.orchestrator.recommend(request)
print(f"Recommendation: {response.recommendation}")
print(f"Confidence: {response.confidence}")
print(f"Reasoning: {response.reasoning}")
```

## üìä Key Features

- **Multi-Agent Coordination**: Manager orchestrates Analyst and Reflector for comprehensive reasoning
- **Reflection Strategies**: Four strategies (none, last_trial, reflexion, combined)
- **Sequential Analysis**: Deep understanding of user interaction patterns
- **Comprehensive Logging**: Full traceability of agent decisions and performance
- **Flexible Datasets**: Support for Beauty and Delivery Hero datasets
- **Robust Evaluation**: Leave-one-out setup with multiple metrics

## üß™ Testing

The system includes comprehensive test coverage across multiple levels:

```bash
# Component-level tests
python tests/test_agents.py              # Agent functionality
python tests/test_datasets.py           # Dataset processing  
python tests/test_pipeline.py           # Pipeline integration
python tests/test_system_integration.py # End-to-end system
python tests/test_api_integration.py    # API integration

# Comprehensive verification
python tests/test_all_components.py     # Full system verification
```

### Test Coverage Details

#### Unit Tests (`tests/test_agents.py`)
- **Manager Agent**: Think/act cycle, action parsing, performance tracking
- **Analyst Agent**: User/item analysis, tool execution, data source updates  
- **Reflector Agent**: Multiple reflection strategies, reflection parsing
- **Base Classes**: Agent types, performance metrics, state management

#### Integration Tests (`tests/test_system_integration.py`)
- **Agent Communication**: Manager ‚Üí Analyst ‚Üí Reflector workflow
- **Multi-agent Coordination**: Complete recommendation pipeline
- **LLM Integration**: MockLLMProvider and GeminiProvider testing
- **End-to-end Workflow**: Request ‚Üí Processing ‚Üí Response cycle

#### Component Tests (`tests/test_all_components.py`)
- **Metrics System**: HR@K, NDCG@K, MRR calculation verification
- **Logging System**: Performance tracking, action logging
- **Dataset Processing**: Synthetic data generation, evaluation splits
- **Orchestrator**: Request handling, response structure validation
- **Complete Pipeline**: Factory functions, demo predictions

Each test file includes specific scenarios:
- ‚úÖ **Functional Correctness**: All components work as designed
- ‚úÖ **Error Handling**: Graceful failure and fallback mechanisms  
- ‚úÖ **Performance Tracking**: Metrics collection and reporting
- ‚úÖ **Integration Points**: Proper component interaction

## üìÅ Project Structure

```
agentic_recommender/
‚îú‚îÄ‚îÄ agents/             # Multi-agent system
‚îÇ   ‚îú‚îÄ‚îÄ base.py         # Base agent classes (Agent, ToolAgent, AgentType, ReflectionStrategy)
‚îÇ   ‚îú‚îÄ‚îÄ manager.py      # Manager agent (two-stage think/act mechanism)
‚îÇ   ‚îú‚îÄ‚îÄ analyst.py      # Analyst agent (user/item analysis with tools)
‚îÇ   ‚îî‚îÄ‚îÄ reflector.py    # Reflector agent (multiple reflection strategies)
‚îú‚îÄ‚îÄ core/               # Orchestration layer
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py # AgentOrchestrator (coordinates multi-agent workflow)
‚îú‚îÄ‚îÄ datasets/           # Data processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ base_dataset.py # SequentialDataset base class
‚îÇ   ‚îî‚îÄ‚îÄ beauty_dataset.py # BeautyDataset & DeliveryHeroDataset
‚îú‚îÄ‚îÄ models/             # LLM provider abstraction
‚îÇ   ‚îî‚îÄ‚îÄ llm_provider.py # LLMProvider, GeminiProvider, MockLLMProvider
‚îú‚îÄ‚îÄ system/             # Complete pipeline integration
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py     # RecommendationPipeline (end-to-end system)
‚îú‚îÄ‚îÄ training/           # Training utilities  
‚îÇ   ‚îî‚îÄ‚îÄ data_preparation.py # Dataset preparation for training
‚îî‚îÄ‚îÄ utils/              # Shared utilities
    ‚îú‚îÄ‚îÄ metrics.py      # Evaluation metrics (HR@K, NDCG@K, MRR)
    ‚îî‚îÄ‚îÄ logging.py      # Performance logging and tracking
```

### Core Components Detail

#### Agents (`agentic_recommender/agents/`)
- **`base.py:51`** - `Agent` abstract base class with performance tracking
- **`base.py:108`** - `ToolAgent` for command execution (used by Analyst)
- **`manager.py:17`** - `Manager` with dual LLM architecture (think_llm + action_llm)
- **`analyst.py:15`** - `Analyst` with data analysis tools (UserInfo, ItemInfo, etc.)
- **`reflector.py:20`** - `Reflector` supporting 4 reflection strategies

#### Core Orchestration (`agentic_recommender/core/`)
- **`orchestrator.py:36`** - `AgentOrchestrator` coordinates agent workflow
- **`orchestrator.py:18`** - `RecommendationRequest` input structure
- **`orchestrator.py:28`** - `RecommendationResponse` output structure

#### System Pipeline (`agentic_recommender/system/`)
- **`pipeline.py:19`** - `RecommendationPipeline` integrates all components
- **`pipeline.py:246`** - `create_pipeline()` factory function

## üéØ Example Use Cases

### 1. E-commerce Recommendation
```python
# Tech product sequence
user_sequence = ["gaming_laptop", "mechanical_keyboard", "gaming_mouse"]  
candidates = ["wireless_headphones", "monitor", "webcam"]

response = pipeline.run_demo_prediction(user_sequence, candidates)
# ‚Üí Recommends "monitor" based on sequential patterns
```

### 2. Beauty Product Recommendation  
```python
# Beauty routine sequence
user_sequence = ["foundation", "concealer", "lipstick"]
candidates = ["mascara", "eyeshadow", "blush"]

response = pipeline.run_demo_prediction(user_sequence, candidates)
# ‚Üí Recommends "mascara" completing the look
```

## üìà Performance Monitoring

The system provides comprehensive performance tracking through multiple layers:

### Agent-Level Metrics (`Agent.get_performance_stats()`)
```python
{
    'agent_type': 'Manager',
    'total_calls': 15,
    'avg_time_per_call': 0.234,
    'step_count': 8,
    'is_finished': True
}
```

### Orchestrator-Level Stats (`AgentOrchestrator.get_system_stats()`)  
```python
{
    'orchestrator': {
        'total_requests': 25,
        'successful_requests': 24,
        'success_rate': 0.96,
        'current_session': 'session_1756034452'
    },
    'agents': {
        'manager': {...},  # Individual agent stats
        'analyst': {...},
        'reflector': {...}
    },
    'communication': {
        'messages_in_session': 12,
        'last_session_id': 'session_1756034452'
    }
}
```

### Logging System (`utils/logging.py`)
- **Action Logging**: Every agent action with context and timing
- **Communication Tracking**: Inter-agent message flow
- **Performance Metrics**: Response times, token usage estimates
- **Session Management**: Conversation history and workflow traces

### Evaluation Framework (`utils/metrics.py`)
- **Hit Rate@K**: `hit_rate_at_k(predictions, ground_truth, k)`
- **NDCG@K**: `ndcg_at_k(predictions, ground_truth, k)` 
- **MRR**: `mrr(predictions, ground_truth)`
- **Batch Evaluation**: `evaluate_recommendations(pred_list, gt_list, k_values)`

### Real-time Monitoring
The system logs all activities to structured JSONL files:
```bash
logs/session_1756034452.jsonl  # Session-specific logs
```

Each log entry includes:
- Timestamp and agent identification
- Action type and parameters
- Execution duration and context
- Performance metrics and metadata

## üîß Configuration

### Reflection Strategies
- `none`: No reflection
- `last_trial`: Store previous attempt context
- `reflexion`: LLM-based reflection and improvement
- `last_trial_and_reflexion`: Combined approach

### LLM Providers
- `GeminiProvider`: Google's Gemini API
- `MockLLMProvider`: For testing and development

### Dataset Options
- `beauty`: Amazon Beauty 5-core dataset
- `delivery_hero`: Food delivery dataset
- Synthetic data generation for testing

## üß† Research Foundation

This implementation combines insights from three cutting-edge research papers:

### 1. **MACRec** - Multi-Agent Collaborative Reasoning
- **Paper**: `papers/MacRec.pdf`
- **Implementation Reference**: `previousWorks/MACRec/`
- **Key Contributions**:
  - Two-stage Manager architecture (think/act mechanism)
  - Tool-based Analyst agent with specialized commands
  - Multiple reflection strategies (none/last_trial/reflexion/combined)
  - Agent orchestration and communication protocols

### 2. **LLM-Sequential-Recommendation**
- **Implementation Reference**: `previousWorks/LLM-Sequential-Recommendation/`  
- **Key Contributions**:
  - Sequential dataset processing pipeline
  - Leave-one-out evaluation methodology
  - Comprehensive evaluation metrics (HR@K, NDCG@K, MRR)
  - Amazon Beauty and delivery datasets integration

### 3. **ThinkRec** - Reasoning-Enhanced Recommendation
- **Paper**: `papers/thinkRec.pdf`
- **Implementation Reference**: `previousWorks/ThinkRec/`
- **Key Contributions**:
  - Dual-objective training (accuracy + reasoning quality)
  - Sequential pattern analysis and reasoning chains
  - User preference understanding through reflection

### Implementation Architecture Mapping

| Research Component | Our Implementation | Code Reference |
|-------------------|-------------------|----------------|
| MACRec Manager | `Manager` class | `agents/manager.py:17` |
| MACRec Analyst | `Analyst` class | `agents/analyst.py:15` |
| MACRec Reflector | `Reflector` class | `agents/reflector.py:20` |
| LLM-SeqRec Datasets | `BeautyDataset` | `datasets/beauty_dataset.py:15` |
| LLM-SeqRec Metrics | Evaluation functions | `utils/metrics.py:10` |
| ThinkRec Reasoning | Orchestrator workflow | `core/orchestrator.py:149` |

### Analysis Documents
- **`MACRec_Analysis.md`**: Comprehensive MACRec analysis and implementation notes
- **`LLM_Sequential_Recommendation_Analysis.md`**: Dataset and evaluation framework analysis  
- **`ThinkRec_Technical_Analysis.md`**: Reasoning and training methodology analysis

## üö¶ Getting Started

1. **Clone and Install**:
   ```bash
   git clone <repository>
   pip install -r requirements.txt
   ```

2. **Set API Key** (optional for real LLM):
   ```bash
   export GEMINI_API_KEY="your-api-key"
   ```

3. **Run Demo**:
   ```bash
   python examples/complete_pipeline_demo.py
   ```

4. **Explore Examples**:
   - `examples/demo_system.py` - Basic agent interaction
   - `examples/complete_pipeline_demo.py` - Full system demo

## üìö Documentation

- `CLAUDE.md` - Project instructions and references
- `MACRec_Analysis.md` - Detailed technical analysis
- `Design_Document.md` - System design and architecture
- `papers/` - Research paper references

## üéâ System Status

‚úÖ **Complete Implementation**:
- Multi-agent system with orchestration
- Dataset processing and evaluation  
- Comprehensive testing and logging
- End-to-end recommendation pipeline
- Reflection-based improvements
- Performance monitoring

The system is **ready for research and production use** with full test coverage and comprehensive documentation.

## üìù Session Notes: LoRA Finetuning Data Flow

Context from `finetune/scripts/finetune_lora.py` on how supervised JSON samples are fed to Qwen during LoRA finetuning:
- Each JSON record provides `system`, `history`, `instruction`, `input`, and `output`; the script turns this into chat turns via `to_chat_messages`.
- The user turn is `instruction` plus `input` (if present); the assistant turn is exactly `output`. Optional `system` and any `history` are prepended.
- `tokenizer.apply_chat_template` renders those turns into Qwen chat text (e.g., `<|im_start|>system ... <|im_end|>`, `<|im_start|>user ...`, `<|im_start|>assistant\nNo<|im_end|>`), no generation prompt added.
- `preprocess_datasets_parallel` appends EOS, tokenizes, and masks all prompt tokens with `-100`, keeping labels only on the assistant response tokens.
- SFTTrainer then trains on these token/label pairs; metrics extract Yes/No from decoded assistant responses to compute accuracy/F1.


## Jan 12 Reform: Modular Recommendation System

This reform introduces a test-first approach with modular components for similarity calculation, enriched data representations, and Top-K evaluation.

**Full design documentation**: `agentic_recommender/agents/doc/ENHANCED_DESIGN_V2.md`

### New Module Structure

```
agentic_recommender/
‚îú‚îÄ‚îÄ data/                       # Data loading and representations
‚îÇ   ‚îú‚îÄ‚îÄ enriched_loader.py      # Singapore dataset loader (preserves full schema)
‚îÇ   ‚îî‚îÄ‚îÄ representations.py      # EnrichedUser, CuisineProfile, CuisineRegistry
‚îú‚îÄ‚îÄ similarity/                 # Modular similarity calculation
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # Abstract SimilarityMethod interface
‚îÇ   ‚îú‚îÄ‚îÄ methods.py              # Swing, Cosine, Jaccard implementations
‚îÇ   ‚îî‚îÄ‚îÄ factory.py              # SimilarityFactory for easy method switching
‚îú‚îÄ‚îÄ evaluation/                 # Top-K evaluation framework
‚îÇ   ‚îî‚îÄ‚îÄ topk.py                 # TopKMetrics, SequentialRecommendationEvaluator
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ llm_provider.py         # Added OpenRouterProvider (Gemini 2.5 Flash)
‚îî‚îÄ‚îÄ tests/                      # Comprehensive test suite (94 tests)
    ‚îú‚îÄ‚îÄ conftest.py             # Pytest fixtures with real Singapore data
    ‚îú‚îÄ‚îÄ test_data_loader.py     # Data loader and representation tests
    ‚îú‚îÄ‚îÄ test_similarity.py      # Similarity method tests
    ‚îú‚îÄ‚îÄ test_llm_provider.py    # LLM provider tests (including API tests)
    ‚îî‚îÄ‚îÄ test_topk_evaluation.py # Top-K evaluation tests
```

### New Files Explained

#### Data Module (`agentic_recommender/data/`)

| File | Purpose |
|------|---------|
| `enriched_loader.py` | `EnrichedDataLoader` - Loads Singapore dataset preserving all fields (customer_id, order_id, geohash, chain_id, vendor_id). Parses timestamps into hour/day_num. |
| `representations.py` | `EnrichedUser` - User representation with cuisine preferences, temporal patterns (peak hours/weekdays), vendor loyalty, basket size. `CuisineProfile` - Cuisine profiles with peak ordering hours/days, meal time distribution. `CuisineRegistry` - Fast lookup for cuisine temporal patterns. |

#### Similarity Module (`agentic_recommender/similarity/`)

| File | Purpose |
|------|---------|
| `base.py` | Abstract `SimilarityMethod` class defining interface: `fit()`, `compute_similarity()`, `get_similar()`. Includes caching and configuration. |
| `methods.py` | Three similarity implementations: **SwingMethod** (Alibaba's anti-noise CF algorithm), **CosineMethod** (vector-based similarity), **JaccardMethod** (set overlap similarity). |
| `factory.py` | `SimilarityFactory` - Factory pattern for creating/switching similarity methods at runtime. Supports `factory.create("swing")`, `factory.set_default("cosine")`. |

#### Evaluation Module (`agentic_recommender/evaluation/`)

| File | Purpose |
|------|---------|
| `topk.py` | `TopKMetrics` - Dataclass for Hit@1/3/5/10, MRR, NDCG metrics. `SequentialRecommendationEvaluator` - Evaluates LLM predictions against ground truth. `TopKTestDataBuilder` - Creates test samples from order history (last order = ground truth). |

#### LLM Provider (`agentic_recommender/models/llm_provider.py`)

| Addition | Purpose |
|----------|---------|
| `OpenRouterProvider` | LLM provider using OpenRouter API with Gemini 2.5 Flash (`google/gemini-2.0-flash-001`). Supports JSON mode, batch generation, and metrics tracking. |

### Test Suite (`agentic_recommender/tests/`)

**94 tests** covering all new modules with real Singapore data:

| File | Tests | Coverage |
|------|-------|----------|
| `conftest.py` | - | Pytest fixtures: `all_orders`, `all_vendors`, `all_products`, `sample_customer_orders`, `merged_orders` |
| `test_data_loader.py` | 29 | EnrichedDataLoader (12), EnrichedUser (7), CuisineProfile (5), CuisineRegistry (5) |
| `test_similarity.py` | 25 | SwingMethod (8), CosineMethod (4), JaccardMethod (4), SimilarityFactory (9) |
| `test_llm_provider.py` | 17 | MockLLMProvider (5), OpenRouterProvider (5), API tests (5), Factory (2) |
| `test_topk_evaluation.py` | 20 | TopKMetrics (3), EvaluationResult (1), TopKTestDataBuilder (5), Evaluator (8), MetricsComputation (3) |

### Running Tests

```bash
# Run all 94 tests
python -m pytest agentic_recommender/tests/ -v

# Run specific test modules
python -m pytest agentic_recommender/tests/test_data_loader.py -v
python -m pytest agentic_recommender/tests/test_similarity.py -v
python -m pytest agentic_recommender/tests/test_llm_provider.py -v
python -m pytest agentic_recommender/tests/test_topk_evaluation.py -v

# Run with coverage
python -m pytest agentic_recommender/tests/ -v --cov=agentic_recommender
```

### Quick Validation Commands

```bash
# Test data loading
python -c "
from agentic_recommender.data import EnrichedDataLoader
loader = EnrichedDataLoader('/Users/zhenkai/Downloads/data_sg')
print(loader.get_stats())
"

# Test similarity methods
python -c "
from agentic_recommender.similarity import SimilarityFactory
print('Available methods:', SimilarityFactory.available_methods())
"

# Test LLM provider (real API call)
python -c "
from agentic_recommender.models.llm_provider import OpenRouterProvider
llm = OpenRouterProvider(api_key='your-api-key')
print(llm.generate('What is 2+2?'))
"

# Run full Top-K evaluation
python agentic_recommender/tests/test_topk_evaluation.py
```

### Key Design Decisions

1. **Top-K Hit Ratio** instead of Yes/No classification - More meaningful for sequential recommendation
2. **Modular Similarity** - Easy to switch between Swing/Cosine/Jaccard via factory pattern
3. **Enriched Representations** - Preserve all original data fields; add temporal patterns (peak hours/days)
4. **Test-First Approach** - 94 tests using real Singapore data fixtures
5. **OpenRouter + Gemini 2.5 Flash** - Fast and cost-effective for testing


## Enhanced Two-Round LLM Reranking System (Stage 8)

A cuisine-level recommendation pipeline with two-round LLM reranking and LightGCN collaborative filtering.

### Pipeline Flow

```
Data Loading ‚Üí Cuisine-Cuisine Swing Candidates (top 20)
    ‚Üí Round 1: LLM Reranking ‚Üí LightGCN User-Cuisine Similarity
    ‚Üí Round 2: Reflection (final reranking) ‚Üí Metrics
```

### Key Components

| Component | File | Description |
|-----------|------|-------------|
| `CuisineSwingMethod` | `similarity/methods.py` | Cuisine-to-cuisine swing similarity for candidate generation |
| `LightGCNEmbeddingManager` | `similarity/lightGCN.py` | User-cuisine embeddings with disk caching |
| `CuisineBasedCandidateGenerator` | `evaluation/rerank_eval.py` | Generates 20 candidates from user's cuisine history |
| `EnhancedRerankEvaluator` | `evaluation/rerank_eval.py` | Two-round LLM evaluation with LightGCN reflection |

### Metrics

- **NDCG@K** (K=5, 10)
- **MRR@K** (K=5, 10)
- **Hit Rate@K** (K=1, 3, 5, 10)
- Round 1 vs Final comparison

### Configuration

Edit `workflow_config_linux.yaml`:

```yaml
run_enhanced_rerank_evaluation:
  enabled: true
  settings:
    n_candidates: 20
    items_per_seed: 5
    dataset_name: "data_se"
    lightgcn_epochs: 50
    lightgcn_embedding_dim: 64
    temperature_round1: 0.3
    temperature_round2: 0.2
    n_samples: 10
    min_history: 5
```

### Run Command

```bash
python -m agentic_recommender.workflow.workflow_runner \
    --config agentic_recommender/workflow/workflow_config_linux.yaml \
    --stages run_enhanced_rerank_evaluation
```

### LightGCN Cache

Embeddings are cached at `~/.cache/agentic_recommender/lightgcn/{dataset_name}_embeddings.pkl` for fast reloading.