{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat Evaluation â€” Prompt & Response Analysis\n",
    "\n",
    "Analyze the Gemini 500-sample repeat evaluation results:\n",
    "- Prompt lengths (Round 1 & Round 2)\n",
    "- Response lengths (Round 1 & Round 2)\n",
    "- LLM timing per round\n",
    "- Candidate counts\n",
    "- Similar user record counts\n",
    "- Order history sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the Gemini 500-sample detailed results\n",
    "DETAILED_PATH = Path(\"../outputs/202601310621/stage9_repeat_detailed.json\")\n",
    "\n",
    "with open(DETAILED_PATH) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Keys: {list(data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics into a DataFrame\n",
    "rows = []\n",
    "for d in data:\n",
    "    r1_prompt = d.get('round1_prompt', '')\n",
    "    r1_resp = d.get('round1_raw_response', '')\n",
    "    r2_prompt = d.get('round2_prompt', '')\n",
    "    r2_resp = d.get('round2_raw_response', '')\n",
    "    \n",
    "    # Count similar user records\n",
    "    sim_users = d.get('similar_users', [])\n",
    "    total_sim_records = sum(u.get('record_count', 0) for u in sim_users)\n",
    "    \n",
    "    rows.append({\n",
    "        'sample_idx': d.get('sample_idx'),\n",
    "        'customer_id': d.get('customer_id'),\n",
    "        'history_size': len(d.get('order_history_tuples', [])),\n",
    "        'r1_prompt_chars': len(r1_prompt),\n",
    "        'r1_prompt_words': len(r1_prompt.split()),\n",
    "        'r1_resp_chars': len(r1_resp),\n",
    "        'r1_resp_words': len(r1_resp.split()),\n",
    "        'r1_llm_ms': d.get('round1_llm_ms', 0),\n",
    "        'r2_prompt_chars': len(r2_prompt),\n",
    "        'r2_prompt_words': len(r2_prompt.split()),\n",
    "        'r2_resp_chars': len(r2_resp),\n",
    "        'r2_resp_words': len(r2_resp.split()),\n",
    "        'r2_llm_ms': d.get('round2_llm_ms', 0),\n",
    "        'candidate_count': d.get('candidate_count', 0),\n",
    "        'similar_users_count': len(sim_users),\n",
    "        'similar_user_records': total_sim_records,\n",
    "        'ground_truth_rank': d.get('ground_truth_rank', 0),\n",
    "        'total_time_ms': d.get('time_ms', 0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key columns to summarize\n",
    "cols = [\n",
    "    'history_size',\n",
    "    'r1_prompt_chars', 'r1_prompt_words', 'r1_resp_chars', 'r1_resp_words', 'r1_llm_ms',\n",
    "    'r2_prompt_chars', 'r2_prompt_words', 'r2_resp_chars', 'r2_resp_words', 'r2_llm_ms',\n",
    "    'candidate_count', 'similar_users_count', 'similar_user_records',\n",
    "    'total_time_ms',\n",
    "]\n",
    "\n",
    "summary = df[cols].describe(percentiles=[.25, .5, .75, .90, .95, .99]).T\n",
    "summary = summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', '90%', '95%', '99%', 'max']]\n",
    "summary = summary.round(1)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Length Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Round 1 prompt chars\n",
    "axes[0, 0].hist(df['r1_prompt_chars'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Round 1 Prompt (chars)')\n",
    "axes[0, 0].axvline(df['r1_prompt_chars'].mean(), color='red', linestyle='--', label=f\"mean={df['r1_prompt_chars'].mean():.0f}\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Round 1 response chars\n",
    "axes[0, 1].hist(df['r1_resp_chars'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Round 1 Response (chars)')\n",
    "axes[0, 1].axvline(df['r1_resp_chars'].mean(), color='red', linestyle='--', label=f\"mean={df['r1_resp_chars'].mean():.0f}\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Round 1 LLM time\n",
    "axes[0, 2].hist(df['r1_llm_ms'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].set_title('Round 1 LLM Time (ms)')\n",
    "axes[0, 2].axvline(df['r1_llm_ms'].mean(), color='red', linestyle='--', label=f\"mean={df['r1_llm_ms'].mean():.0f}ms\")\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Round 2 prompt chars\n",
    "axes[1, 0].hist(df['r2_prompt_chars'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Round 2 Prompt (chars)')\n",
    "axes[1, 0].axvline(df['r2_prompt_chars'].mean(), color='red', linestyle='--', label=f\"mean={df['r2_prompt_chars'].mean():.0f}\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Round 2 response chars\n",
    "axes[1, 1].hist(df['r2_resp_chars'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title('Round 2 Response (chars)')\n",
    "axes[1, 1].axvline(df['r2_resp_chars'].mean(), color='red', linestyle='--', label=f\"mean={df['r2_resp_chars'].mean():.0f}\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Round 2 LLM time\n",
    "axes[1, 2].hist(df['r2_llm_ms'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 2].set_title('Round 2 LLM Time (ms)')\n",
    "axes[1, 2].axvline(df['r2_llm_ms'].mean(), color='red', linestyle='--', label=f\"mean={df['r2_llm_ms'].mean():.0f}ms\")\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../resultExploration/repeat_prompt_response_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate & Similar User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Candidate count distribution\n",
    "axes[0].hist(df['candidate_count'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Candidate Vendors per Sample')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].axvline(df['candidate_count'].mean(), color='red', linestyle='--', label=f\"mean={df['candidate_count'].mean():.1f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Similar user records\n",
    "axes[1].hist(df['similar_user_records'], bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1].set_title('Similar User Records per Sample')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].axvline(df['similar_user_records'].mean(), color='red', linestyle='--', label=f\"mean={df['similar_user_records'].mean():.1f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Order history size\n",
    "axes[2].hist(df['history_size'], bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "axes[2].set_title('Order History Size')\n",
    "axes[2].set_xlabel('Count')\n",
    "axes[2].axvline(df['history_size'].mean(), color='red', linestyle='--', label=f\"mean={df['history_size'].mean():.1f}\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../resultExploration/repeat_candidate_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Length vs. LLM Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Round 1: prompt length vs time\n",
    "axes[0].scatter(df['r1_prompt_chars'], df['r1_llm_ms'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('Round 1 Prompt Length (chars)')\n",
    "axes[0].set_ylabel('Round 1 LLM Time (ms)')\n",
    "axes[0].set_title('R1: Prompt Length vs Response Time')\n",
    "\n",
    "# Round 2: prompt length vs time\n",
    "axes[1].scatter(df['r2_prompt_chars'], df['r2_llm_ms'], alpha=0.3, s=10, color='orange')\n",
    "axes[1].set_xlabel('Round 2 Prompt Length (chars)')\n",
    "axes[1].set_ylabel('Round 2 LLM Time (ms)')\n",
    "axes[1].set_title('R2: Prompt Length vs Response Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../resultExploration/repeat_prompt_vs_time.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Rank Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rank distribution (exclude 0 = not found)\n",
    "found = df[df['ground_truth_rank'] > 0]\n",
    "not_found = df[df['ground_truth_rank'] == 0]\n",
    "\n",
    "axes[0].hist(found['ground_truth_rank'], bins=range(1, found['ground_truth_rank'].max()+2),\n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'GT Rank Distribution (found: {len(found)}/{len(df)})')\n",
    "axes[0].set_xlabel('Rank')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Hit@K cumulative\n",
    "k_values = list(range(1, 21))\n",
    "hit_rates = []\n",
    "for k in k_values:\n",
    "    hit = (found['ground_truth_rank'] <= k).sum() / len(df)\n",
    "    hit_rates.append(hit)\n",
    "\n",
    "axes[1].plot(k_values, hit_rates, 'o-', markersize=4)\n",
    "axes[1].set_title('Cumulative Hit Rate')\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Hit@K')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate key points\n",
    "for k in [1, 3, 5]:\n",
    "    axes[1].annotate(f'Hit@{k}={hit_rates[k-1]:.3f}',\n",
    "                     xy=(k, hit_rates[k-1]),\n",
    "                     xytext=(k+1, hit_rates[k-1]-0.05),\n",
    "                     fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../resultExploration/repeat_rank_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Content Analysis\n",
    "\n",
    "Check for thinking tokens / excessively long responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for thinking tokens in responses\n",
    "r1_has_think = sum(1 for d in data if '<think>' in d.get('round1_raw_response', ''))\n",
    "r2_has_think = sum(1 for d in data if '<think>' in d.get('round2_raw_response', ''))\n",
    "print(f\"Round 1 responses with <think> tags: {r1_has_think}/{len(data)}\")\n",
    "print(f\"Round 2 responses with <think> tags: {r2_has_think}/{len(data)}\")\n",
    "\n",
    "# Find longest responses\n",
    "print(\"\\n--- Top 5 Longest Round 1 Responses (chars) ---\")\n",
    "top_r1 = sorted(data, key=lambda d: len(d.get('round1_raw_response', '')), reverse=True)[:5]\n",
    "for d in top_r1:\n",
    "    r = d.get('round1_raw_response', '')\n",
    "    print(f\"  Sample {d['sample_idx']}: {len(r)} chars, {len(r.split())} words\")\n",
    "\n",
    "print(\"\\n--- Top 5 Longest Round 2 Responses (chars) ---\")\n",
    "top_r2 = sorted(data, key=lambda d: len(d.get('round2_raw_response', '')), reverse=True)[:5]\n",
    "for d in top_r2:\n",
    "    r = d.get('round2_raw_response', '')\n",
    "    print(f\"  Sample {d['sample_idx']}: {len(r)} chars, {len(r.split())} words\")\n",
    "\n",
    "# Estimate token counts (rough: chars / 4)\n",
    "print(\"\\n--- Estimated Token Counts (chars/4 approximation) ---\")\n",
    "r1_tokens = [len(d.get('round1_prompt', '')) / 4 + len(d.get('round1_raw_response', '')) / 4 for d in data]\n",
    "r2_tokens = [len(d.get('round2_prompt', '')) / 4 + len(d.get('round2_raw_response', '')) / 4 for d in data]\n",
    "total_tokens = [r1 + r2 for r1, r2 in zip(r1_tokens, r2_tokens)]\n",
    "\n",
    "print(f\"R1 avg tokens: {np.mean(r1_tokens):.0f} (input+output)\")\n",
    "print(f\"R2 avg tokens: {np.mean(r2_tokens):.0f} (input+output)\")\n",
    "print(f\"Total avg tokens/sample: {np.mean(total_tokens):.0f}\")\n",
    "print(f\"Total tokens (all samples): {np.sum(total_tokens):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing breakdown\n",
    "print(\"=\" * 60)\n",
    "print(\"TIMING BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Round 1 LLM time:  mean={df['r1_llm_ms'].mean():.0f}ms, median={df['r1_llm_ms'].median():.0f}ms, p95={df['r1_llm_ms'].quantile(0.95):.0f}ms\")\n",
    "print(f\"Round 2 LLM time:  mean={df['r2_llm_ms'].mean():.0f}ms, median={df['r2_llm_ms'].median():.0f}ms, p95={df['r2_llm_ms'].quantile(0.95):.0f}ms\")\n",
    "print(f\"Total time:        mean={df['total_time_ms'].mean():.0f}ms, median={df['total_time_ms'].median():.0f}ms, p95={df['total_time_ms'].quantile(0.95):.0f}ms\")\n",
    "\n",
    "# Non-LLM overhead\n",
    "df['overhead_ms'] = df['total_time_ms'] - df['r1_llm_ms'] - df['r2_llm_ms']\n",
    "print(f\"Overhead (non-LLM): mean={df['overhead_ms'].mean():.0f}ms, median={df['overhead_ms'].median():.0f}ms\")\n",
    "print(f\"LLM % of total:    {((df['r1_llm_ms'] + df['r2_llm_ms']) / df['total_time_ms']).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of History Size on Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin history sizes and compute hit@1 per bin\n",
    "df['history_bin'] = pd.cut(df['history_size'], bins=[0, 5, 10, 20, 50, 100, 500], labels=['1-5', '6-10', '11-20', '21-50', '51-100', '100+'])\n",
    "\n",
    "bin_stats = df.groupby('history_bin', observed=True).agg(\n",
    "    count=('sample_idx', 'count'),\n",
    "    hit1=('ground_truth_rank', lambda x: (x == 1).sum() / len(x)),\n",
    "    hit3=('ground_truth_rank', lambda x: ((x > 0) & (x <= 3)).sum() / len(x)),\n",
    "    hit5=('ground_truth_rank', lambda x: ((x > 0) & (x <= 5)).sum() / len(x)),\n",
    "    avg_r1_prompt=('r1_prompt_chars', 'mean'),\n",
    "    avg_r2_prompt=('r2_prompt_chars', 'mean'),\n",
    "    avg_time=('total_time_ms', 'mean'),\n",
    ").round(3)\n",
    "\n",
    "print(\"Performance by History Size:\")\n",
    "print(bin_stats.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}