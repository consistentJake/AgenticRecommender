{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Repeat Dataset Evaluation Analysis (Stage 9)\n\nThis notebook analyzes the repeat order evaluation results from `stage9_repeat_detailed.json`.\nThe pipeline predicts which vendor a user will re-order from using a two-round LLM approach:\n- **Round 1**: Predict top-3 cuisines (using order history + LightGCN CF scores)\n- **Round 2**: Rank candidate vendors within those cuisines\n\n**Key Questions:**\n1. How well does the pipeline predict repeat orders? (Hit@K, NDCG, MRR)\n2. How often is the ground truth vendor in the candidate list?\n3. How accurate is Round 1 cuisine prediction?\n4. Which cuisines are hardest/easiest to predict?\n5. Do temporal patterns (time of day, day of week) affect performance?\n6. What characterizes failure cases?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:50.644244Z",
     "iopub.status.busy": "2026-01-27T09:02:50.644141Z",
     "iopub.status.idle": "2026-01-27T09:02:50.937008Z",
     "shell.execute_reply": "2026-01-27T09:02:50.936819Z"
    }
   },
   "outputs": [],
   "source": "import json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nfrom IPython.display import display, Markdown\n\n# Load the data\ndata_path = '/home/zhenkai/personal/Projects/AgenticRecommender/outputs/202601310621/stage9_repeat_detailed.json'\nwith open(data_path) as f:\n    data = json.load(f)\n\nprint(f\"Total records: {len(data)}\")\nprint(f\"Fields per record: {list(data[0].keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:50.950906Z",
     "iopub.status.busy": "2026-01-27T09:02:50.950713Z",
     "iopub.status.idle": "2026-01-27T09:02:50.959755Z",
     "shell.execute_reply": "2026-01-27T09:02:50.959614Z"
    }
   },
   "outputs": [],
   "source": "# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Derived columns\ndf['gt_in_candidates'] = df.apply(lambda r: r['ground_truth'] in r['candidate_vendors'], axis=1)\ndf['gt_cuisine_in_round1'] = df.apply(lambda r: r['ground_truth_cuisine'] in r['round1_predicted_cuisines'], axis=1)\ndf['history_len'] = df['order_history_tuples'].apply(len)\ndf['history_cuisines'] = df['order_history_tuples'].apply(lambda tuples: [t[1] for t in tuples])\ndf['history_unique_cuisines'] = df['history_cuisines'].apply(lambda c: len(set(c)))\ndf['lightgcn_gt_cuisine_rank'] = df.apply(\n    lambda r: next((i+1 for i, (c, _) in enumerate(r['lightgcn_top_cuisines']) if c == r['ground_truth_cuisine']), 0),\n    axis=1\n)\n\nprint(\"Columns:\", list(df.columns))\nprint(f\"\\nBasic Statistics (ground_truth_rank, 0 = not found):\")\nprint(df['ground_truth_rank'].describe())\nprint(f\"\\nCandidate count stats:\")\nprint(df['candidate_count'].describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Case Lookup System ────────────────────────────────────────\n# Two hashmaps for instant lookup by sample_idx or customer_id.\n#\n#   lookup(38)                     -> pretty-print case #38\n#   case_by_idx[38]                -> raw dict for case #38\n#   lookup_customer('abc...')      -> pretty-print all cases for a customer\n#   cases_by_customer['abc...']    -> raw list of dicts for a customer\n\ncase_by_idx = {r['sample_idx']: r for r in data}\n\n_by_cust = defaultdict(list)\nfor r in data:\n    _by_cust[r['customer_id']].append(r)\ncases_by_customer = dict(_by_cust)\n\ndef lookup(sample_idx):\n    \"\"\"Pretty-print a single test case by sample_idx. Returns the raw dict.\"\"\"\n    c = case_by_idx.get(sample_idx)\n    if c is None:\n        print(f\"No case with sample_idx={sample_idx}\")\n        return None\n    print(f\"Sample {c['sample_idx']}  |  Customer: {c['customer_id']}\")\n    print(f\"  Ground truth:       {c['ground_truth']}  (cuisine: {c['ground_truth_cuisine']})\")\n    print(f\"  Target time:        day={c['target_day_of_week']}  hour={c['target_hour']}\")\n    print(f\"  GT in candidates:   {c['ground_truth'] in c['candidate_vendors']}\")\n    print(f\"  Ground truth rank:  {c['ground_truth_rank']}  {'(NOT FOUND)' if c['ground_truth_rank'] == 0 else ''}\")\n    print(f\"  Candidate count:    {c['candidate_count']}\")\n    print(f\"\\n  Order history ({len(c['order_history_tuples'])} orders):\")\n    for vendor, cuisine, time_str in c['order_history_tuples']:\n        tag = \"  << GT vendor\" if vendor == c['ground_truth_vendor_id'] else \"\"\n        print(f\"    {vendor}||{cuisine} ({time_str}){tag}\")\n    print(f\"\\n  Round 1 predicted cuisines: {c['round1_predicted_cuisines']}\")\n    print(f\"  GT cuisine in R1:          {c['ground_truth_cuisine'] in c['round1_predicted_cuisines']}\")\n    print(f\"\\n  LightGCN top cuisines:\")\n    for cuisine, score in c['lightgcn_top_cuisines'][:5]:\n        tag = \"  << GT cuisine\" if cuisine == c['ground_truth_cuisine'] else \"\"\n        print(f\"    {cuisine}: {score:.3f}{tag}\")\n    print(f\"\\n  Candidates: {c['candidate_vendors'][:10]}{'...' if len(c['candidate_vendors']) > 10 else ''}\")\n    print(f\"  Final ranking: {c['final_ranking'][:10]}{'...' if len(c['final_ranking']) > 10 else ''}\")\n    print(f\"\\n  Timing: R1={c['round1_llm_ms']:.0f}ms  R2={c['round2_llm_ms']:.0f}ms  Total={c['time_ms']:.0f}ms\")\n    return c\n\ndef lookup_customer(customer_id):\n    \"\"\"Pretty-print all test cases for a given customer_id. Returns list of raw dicts.\"\"\"\n    cases = cases_by_customer.get(customer_id, [])\n    if not cases:\n        print(f\"No cases for customer_id={customer_id}\")\n        return []\n    print(f\"{len(cases)} case(s) for customer {customer_id}:\")\n    for c in cases:\n        gt_in = c['ground_truth'] in c['candidate_vendors']\n        print(f\"  idx={c['sample_idx']}  GT={c['ground_truth']}  \"\n              f\"rank={c['ground_truth_rank']}  GT_in_cands={gt_in}\")\n    return cases\n\nprint(f\"Lookup indices ready:\")\nprint(f\"  case_by_idx       -> {len(case_by_idx)} cases (e.g., lookup(38))\")\nprint(f\"  cases_by_customer -> {len(cases_by_customer)} unique customers (e.g., lookup_customer('...'))\")\nprint(f\"\\nSample customer IDs: {list(cases_by_customer.keys())[:5]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:50.960501Z",
     "iopub.status.busy": "2026-01-27T09:02:50.960417Z",
     "iopub.status.idle": "2026-01-27T09:02:51.141316Z",
     "shell.execute_reply": "2026-01-27T09:02:51.141145Z"
    }
   },
   "outputs": [],
   "source": "# Aggregate metrics\nfound = df[df['ground_truth_rank'] > 0]\ntotal = len(df)\n\nprint(\"=\" * 60)\nprint(\"AGGREGATE METRICS\")\nprint(\"=\" * 60)\n\nfor k in [1, 3, 5]:\n    hit = (found['ground_truth_rank'] <= k).sum() / total\n    print(f\"  Hit@{k}:  {hit*100:.1f}%  ({(found['ground_truth_rank'] <= k).sum()}/{total})\")\n\n# NDCG\nndcg_vals = []\nfor _, r in df.iterrows():\n    rank = r['ground_truth_rank']\n    ndcg_vals.append(1.0 / np.log2(rank + 1) if rank > 0 else 0.0)\ndf['ndcg'] = ndcg_vals\n\n# MRR\nmrr_vals = [1.0 / r if r > 0 else 0.0 for r in df['ground_truth_rank']]\ndf['mrr'] = mrr_vals\n\nprint(f\"\\n  MRR:    {df['mrr'].mean():.4f}\")\nprint(f\"  NDCG@1: {df[df['ground_truth_rank'] <= 1]['ndcg'].sum() / total:.4f}\")\nprint(f\"  NDCG@3: {np.mean([1/np.log2(r+1) if 0 < r <= 3 else 0 for r in df['ground_truth_rank']]):.4f}\")\nprint(f\"  NDCG@5: {np.mean([1/np.log2(r+1) if 0 < r <= 5 else 0 for r in df['ground_truth_rank']]):.4f}\")\n\nprint(f\"\\n  GT found rate:      {len(found)}/{total} ({len(found)/total*100:.1f}%)\")\nprint(f\"  Avg rank (found):   {found['ground_truth_rank'].mean():.2f}\")\nprint(f\"  Avg candidates:     {df['candidate_count'].mean():.1f}\")\nprint(f\"  Avg time (ms):      {df['time_ms'].mean():.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rank distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Full distribution including rank 0 (not found)\nrank_counts = df['ground_truth_rank'].value_counts().sort_index()\ncolors = ['#e74c3c' if r == 0 else '#2ecc71' if r == 1 else '#3498db' for r in rank_counts.index]\naxes[0].bar(rank_counts.index, rank_counts.values, color=colors, edgecolor='black', alpha=0.8)\naxes[0].set_xlabel('Ground Truth Rank (0 = not found)')\naxes[0].set_ylabel('Count')\naxes[0].set_title(f'Rank Distribution (all {len(df)} samples)')\naxes[0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n\n# Right: Only found cases (rank > 0)\nfound_ranks = found['ground_truth_rank'].value_counts().sort_index()\ncolors2 = ['#2ecc71' if r == 1 else '#3498db' for r in found_ranks.index]\naxes[1].bar(found_ranks.index, found_ranks.values, color=colors2, edgecolor='black', alpha=0.8)\naxes[1].set_xlabel('Ground Truth Rank')\naxes[1].set_ylabel('Count')\naxes[1].set_title(f'Rank Distribution (found cases only, n={len(found)})\\nMean rank: {found[\"ground_truth_rank\"].mean():.2f}')\n\nplt.tight_layout()\nplt.show()\n\n# Hit rates\nnot_found = (df['ground_truth_rank'] == 0).sum()\nprint(f\"Rank breakdown:\")\nprint(f\"  Not found (rank 0): {not_found} ({not_found/len(df)*100:.1f}%)\")\nprint(f\"  Rank 1 (Hit@1):     {(df['ground_truth_rank'] == 1).sum()} ({(df['ground_truth_rank'] == 1).mean()*100:.1f}%)\")\nprint(f\"  Rank 2-3:           {((df['ground_truth_rank'] >= 2) & (df['ground_truth_rank'] <= 3)).sum()}\")\nprint(f\"  Rank 4-5:           {((df['ground_truth_rank'] >= 4) & (df['ground_truth_rank'] <= 5)).sum()}\")\nprint(f\"  Rank 6+:            {(df['ground_truth_rank'] > 5).sum()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.141993Z",
     "iopub.status.busy": "2026-01-27T09:02:51.141927Z",
     "iopub.status.idle": "2026-01-27T09:02:51.160082Z",
     "shell.execute_reply": "2026-01-27T09:02:51.159930Z"
    }
   },
   "outputs": [],
   "source": "# Ground truth in candidates analysis\ngt_in = df[df['gt_in_candidates']]\ngt_not_in = df[~df['gt_in_candidates']]\n\nprint(f\"Ground Truth in Candidate Vendors:\")\nprint(f\"  In candidates:     {len(gt_in)} ({len(gt_in)/len(df)*100:.1f}%)  -> gt_in\")\nprint(f\"  NOT in candidates: {len(gt_not_in)} ({len(gt_not_in)/len(df)*100:.1f}%)  -> gt_not_in\")\n\n# Performance comparison\nprint(f\"\\nPerformance when GT IS in candidates:\")\nfor k in [1, 3, 5]:\n    hit = (gt_in['ground_truth_rank'] <= k).sum() / len(gt_in)\n    print(f\"  Hit@{k}: {hit*100:.1f}%\")\n\nprint(f\"\\nPerformance when GT is NOT in candidates:\")\nfor k in [1, 3, 5]:\n    hit = (gt_not_in['ground_truth_rank'] <= k).sum() / len(gt_not_in)\n    print(f\"  Hit@{k}: {hit*100:.1f}%\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].pie([len(gt_in), len(gt_not_in)],\n       labels=['In candidates', 'Not in candidates'],\n       autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\naxes[0].set_title('Ground Truth Vendor in Candidates?')\n\n# Hit@1 comparison\nhit1_in = (gt_in['ground_truth_rank'] == 1).mean() * 100\nhit1_not = (gt_not_in['ground_truth_rank'] == 1).mean() * 100\nbars = axes[1].bar(['GT in candidates', 'GT not in candidates'], [hit1_in, hit1_not],\n                   color=['#2ecc71', '#e74c3c'])\naxes[1].set_ylabel('Hit@1 (%)')\naxes[1].set_title('Hit@1 by GT Presence in Candidates')\nfor bar, val in zip(bars, [hit1_in, hit1_not]):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', ha='center')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.160714Z",
     "iopub.status.busy": "2026-01-27T09:02:51.160650Z",
     "iopub.status.idle": "2026-01-27T09:02:51.163115Z",
     "shell.execute_reply": "2026-01-27T09:02:51.162925Z"
    }
   },
   "outputs": [],
   "source": "## 3. Round 1 Cuisine Prediction Accuracy\n\nRound 1 predicts the top-3 cuisines. If the ground truth cuisine isn't predicted, the correct vendor can't appear in candidates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Round 1 cuisine prediction accuracy\nr1_correct = df[df['gt_cuisine_in_round1']]\nr1_wrong = df[~df['gt_cuisine_in_round1']]\n\nprint(f\"Round 1 Cuisine Prediction:\")\nprint(f\"  GT cuisine in top-3: {len(r1_correct)} ({len(r1_correct)/len(df)*100:.1f}%)\")\nprint(f\"  GT cuisine missed:   {len(r1_wrong)} ({len(r1_wrong)/len(df)*100:.1f}%)\")\n\n# When cuisine is missed, can GT still be found?\nr1_wrong_but_found = r1_wrong[r1_wrong['ground_truth_rank'] > 0]\nprint(f\"\\n  When cuisine missed, GT vendor still found: {len(r1_wrong_but_found)}/{len(r1_wrong)}\")\n\n# Performance split\nprint(f\"\\nPerformance when GT cuisine IS predicted:\")\nfor k in [1, 3, 5]:\n    hit = (r1_correct['ground_truth_rank'].between(1, k)).sum() / len(df)\n    print(f\"  Hit@{k}: {hit*100:.1f}% (of total)\")\n\nprint(f\"\\nPerformance when GT cuisine is NOT predicted:\")\nfor k in [1, 3, 5]:\n    hit = (r1_wrong['ground_truth_rank'].between(1, k)).sum() / len(df)\n    print(f\"  Hit@{k}: {hit*100:.1f}% (of total)\")\n\n# LightGCN cuisine ranking vs Round 1 prediction\nprint(f\"\\nLightGCN ranking of GT cuisine:\")\nprint(f\"  GT cuisine in LightGCN top-3: {(df['lightgcn_gt_cuisine_rank'].between(1,3)).sum()}\")\nprint(f\"  GT cuisine in LightGCN top-5: {(df['lightgcn_gt_cuisine_rank'].between(1,5)).sum()}\")\nprint(f\"  GT cuisine not in top-10:     {(df['lightgcn_gt_cuisine_rank'] == 0).sum()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.163914Z",
     "iopub.status.busy": "2026-01-27T09:02:51.163807Z",
     "iopub.status.idle": "2026-01-27T09:02:51.203390Z",
     "shell.execute_reply": "2026-01-27T09:02:51.203177Z"
    }
   },
   "outputs": [],
   "source": "# Confusion: what cuisines does Round 1 predict when it misses the GT cuisine?\nprint(\"When Round 1 misses the GT cuisine, what was predicted vs actual:\")\nprint(\"=\" * 60)\n\nmissed_cuisine_counter = Counter()\nfor _, row in r1_wrong.iterrows():\n    missed_cuisine_counter[row['ground_truth_cuisine']] += 1\n\nprint(f\"\\nMost frequently missed GT cuisines:\")\nfor cuisine, count in missed_cuisine_counter.most_common(10):\n    total_for_cuisine = (df['ground_truth_cuisine'] == cuisine).sum()\n    print(f\"  {cuisine}: missed {count}/{total_for_cuisine} ({count/total_for_cuisine*100:.0f}%)\")\n\n# Show a few examples\nprint(f\"\\nExamples of missed cuisine predictions (showing 3):\")\nfor _, row in r1_wrong.head(3).iterrows():\n    history_cuisines = [t[1] for t in row['order_history_tuples']]\n    print(f\"\\n  Sample {row['sample_idx']}:\")\n    print(f\"    GT cuisine:  {row['ground_truth_cuisine']}\")\n    print(f\"    Predicted:   {row['round1_predicted_cuisines']}\")\n    print(f\"    History:     {history_cuisines}\")\n    lgcn_rank = row['lightgcn_gt_cuisine_rank']\n    print(f\"    LightGCN rank of GT cuisine: {lgcn_rank if lgcn_rank > 0 else 'not in top 10'}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.204287Z",
     "iopub.status.busy": "2026-01-27T09:02:51.204149Z",
     "iopub.status.idle": "2026-01-27T09:02:51.206941Z",
     "shell.execute_reply": "2026-01-27T09:02:51.206794Z"
    }
   },
   "outputs": [],
   "source": "## 4. Per-Cuisine Performance Breakdown"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.207520Z",
     "iopub.status.busy": "2026-01-27T09:02:51.207457Z",
     "iopub.status.idle": "2026-01-27T09:02:51.209315Z",
     "shell.execute_reply": "2026-01-27T09:02:51.209186Z"
    }
   },
   "outputs": [],
   "source": "# Per-cuisine performance\ncuisine_stats = []\nfor cuisine in df['ground_truth_cuisine'].unique():\n    subset = df[df['ground_truth_cuisine'] == cuisine]\n    n = len(subset)\n    if n < 3:\n        continue\n    hit1 = (subset['ground_truth_rank'] == 1).mean()\n    hit3 = (subset['ground_truth_rank'].between(1, 3)).mean()\n    hit5 = (subset['ground_truth_rank'].between(1, 5)).mean()\n    found_rate = (subset['ground_truth_rank'] > 0).mean()\n    cuisine_in_r1 = subset['gt_cuisine_in_round1'].mean()\n    avg_rank_found = subset[subset['ground_truth_rank'] > 0]['ground_truth_rank'].mean() if (subset['ground_truth_rank'] > 0).any() else float('nan')\n    cuisine_stats.append({\n        'cuisine': cuisine, 'n': n,\n        'hit@1': hit1, 'hit@3': hit3, 'hit@5': hit5,\n        'found_rate': found_rate, 'cuisine_in_r1': cuisine_in_r1,\n        'avg_rank_found': avg_rank_found\n    })\n\ncuisine_df = pd.DataFrame(cuisine_stats).sort_values('hit@1', ascending=False)\n\nprint(\"Per-Cuisine Performance (sorted by Hit@1, min 3 samples):\")\nprint(\"=\" * 100)\nprint(f\"{'Cuisine':<15} {'N':>4}  {'Hit@1':>6}  {'Hit@3':>6}  {'Hit@5':>6}  {'Found%':>7}  {'R1 Cuisine%':>11}  {'AvgRank':>7}\")\nprint(\"-\" * 100)\nfor _, row in cuisine_df.iterrows():\n    print(f\"{row['cuisine']:<15} {row['n']:>4}  {row['hit@1']*100:>5.1f}%  {row['hit@3']*100:>5.1f}%  {row['hit@5']*100:>5.1f}%  {row['found_rate']*100:>6.1f}%  {row['cuisine_in_r1']*100:>10.1f}%  {row['avg_rank_found']:>7.2f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize top cuisines by Hit@1\ntop_cuisines = cuisine_df[cuisine_df['n'] >= 5].head(15)\n\nfig, ax = plt.subplots(figsize=(12, 5))\nx = range(len(top_cuisines))\nwidth = 0.3\n\nax.bar([i - width for i in x], top_cuisines['hit@1'] * 100, width, label='Hit@1', color='#2ecc71')\nax.bar([i for i in x], top_cuisines['hit@3'] * 100, width, label='Hit@3', color='#3498db')\nax.bar([i + width for i in x], top_cuisines['hit@5'] * 100, width, label='Hit@5', color='#9b59b6')\n\nax.set_xlabel('Cuisine')\nax.set_ylabel('Hit Rate (%)')\nax.set_title('Hit@K by Cuisine (min 5 samples)')\nax.set_xticks(x)\nax.set_xticklabels([f\"{r['cuisine']}\\n(n={r['n']})\" for _, r in top_cuisines.iterrows()], rotation=45, ha='right')\nax.legend()\nax.set_ylim(0, 105)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.209812Z",
     "iopub.status.busy": "2026-01-27T09:02:51.209755Z",
     "iopub.status.idle": "2026-01-27T09:02:51.211818Z",
     "shell.execute_reply": "2026-01-27T09:02:51.211689Z"
    }
   },
   "outputs": [],
   "source": "## 5. Temporal Patterns\n\nDoes performance vary by time of day or day of week?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.212448Z",
     "iopub.status.busy": "2026-01-27T09:02:51.212303Z",
     "iopub.status.idle": "2026-01-27T09:02:51.214934Z",
     "shell.execute_reply": "2026-01-27T09:02:51.214778Z"
    }
   },
   "outputs": [],
   "source": "# Temporal analysis\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# By hour\nhour_stats = df.groupby('target_hour').agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean()),\n    hit3=('ground_truth_rank', lambda x: (x.between(1, 3)).mean()),\n    found=('ground_truth_rank', lambda x: (x > 0).mean())\n).reset_index()\n\nax = axes[0]\nax.bar(hour_stats['target_hour'], hour_stats['hit1'] * 100, color='#2ecc71', alpha=0.7, label='Hit@1')\nax.plot(hour_stats['target_hour'], hour_stats['hit3'] * 100, 'o-', color='#3498db', label='Hit@3')\nax.set_xlabel('Target Hour')\nax.set_ylabel('Hit Rate (%)')\nax.set_title('Performance by Hour of Day')\nax.legend()\n# Add sample counts\nfor _, row in hour_stats.iterrows():\n    ax.text(row['target_hour'], row['hit1'] * 100 + 2, f\"n={int(row['n'])}\", ha='center', fontsize=7)\n\n# By day of week\nday_names = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\nday_stats = df.groupby('target_day_of_week').agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean()),\n    hit3=('ground_truth_rank', lambda x: (x.between(1, 3)).mean()),\n    found=('ground_truth_rank', lambda x: (x > 0).mean())\n).reset_index()\n\nax = axes[1]\nax.bar(day_stats['target_day_of_week'], day_stats['hit1'] * 100, color='#2ecc71', alpha=0.7, label='Hit@1')\nax.plot(day_stats['target_day_of_week'], day_stats['hit3'] * 100, 'o-', color='#3498db', label='Hit@3')\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hit Rate (%)')\nax.set_title('Performance by Day of Week')\nax.set_xticks(list(day_names.keys()))\nax.set_xticklabels([day_names[d] for d in sorted(day_names.keys())])\nax.legend()\nfor _, row in day_stats.iterrows():\n    ax.text(row['target_day_of_week'], row['hit1'] * 100 + 2, f\"n={int(row['n'])}\", ha='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.215463Z",
     "iopub.status.busy": "2026-01-27T09:02:51.215392Z",
     "iopub.status.idle": "2026-01-27T09:02:51.218452Z",
     "shell.execute_reply": "2026-01-27T09:02:51.218314Z"
    }
   },
   "outputs": [],
   "source": "## 6. Order History Analysis\n\nHow does the user's order history (length, diversity, repeat frequency) affect prediction accuracy?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.218926Z",
     "iopub.status.busy": "2026-01-27T09:02:51.218866Z",
     "iopub.status.idle": "2026-01-27T09:02:51.351317Z",
     "shell.execute_reply": "2026-01-27T09:02:51.351125Z"
    }
   },
   "outputs": [],
   "source": "# Order history analysis\n# How many times does the GT vendor appear in order history?\ndf['gt_vendor_in_history'] = df.apply(\n    lambda r: sum(1 for t in r['order_history_tuples'] if t[0] == r['ground_truth_vendor_id']),\n    axis=1\n)\n# How many times does the GT cuisine appear in history?\ndf['gt_cuisine_in_history'] = df.apply(\n    lambda r: sum(1 for t in r['order_history_tuples'] if t[1] == r['ground_truth_cuisine']),\n    axis=1\n)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# By GT vendor repeat count in history\nvendor_repeat_stats = df.groupby('gt_vendor_in_history').agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean())\n).reset_index()\n\nax = axes[0]\nax.bar(vendor_repeat_stats['gt_vendor_in_history'], vendor_repeat_stats['hit1'] * 100,\n       color='#9b59b6', edgecolor='black', alpha=0.8)\nax.set_xlabel('# Times GT Vendor in History')\nax.set_ylabel('Hit@1 (%)')\nax.set_title('Hit@1 by GT Vendor Repeat Count')\nfor _, row in vendor_repeat_stats.iterrows():\n    ax.text(row['gt_vendor_in_history'], row['hit1'] * 100 + 1, f\"n={int(row['n'])}\", ha='center', fontsize=8)\n\n# By unique cuisine count in history\ncuisine_div_stats = df.groupby('history_unique_cuisines').agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean())\n).reset_index()\n\nax = axes[1]\nax.bar(cuisine_div_stats['history_unique_cuisines'], cuisine_div_stats['hit1'] * 100,\n       color='#e67e22', edgecolor='black', alpha=0.8)\nax.set_xlabel('# Unique Cuisines in History')\nax.set_ylabel('Hit@1 (%)')\nax.set_title('Hit@1 by History Cuisine Diversity')\nfor _, row in cuisine_div_stats.iterrows():\n    ax.text(row['history_unique_cuisines'], row['hit1'] * 100 + 1, f\"n={int(row['n'])}\", ha='center', fontsize=8)\n\n# By candidate count\nbins = [0, 3, 5, 10, 15, 21]\ndf['cand_bin'] = pd.cut(df['candidate_count'], bins=bins, labels=['1-3', '4-5', '6-10', '11-15', '16-20'])\ncand_stats = df.groupby('cand_bin', observed=True).agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean()),\n    found=('ground_truth_rank', lambda x: (x > 0).mean())\n).reset_index()\n\nax = axes[2]\nx = range(len(cand_stats))\nax.bar(x, cand_stats['hit1'] * 100, color='#1abc9c', edgecolor='black', alpha=0.8)\nax.set_xlabel('Candidate Count')\nax.set_ylabel('Hit@1 (%)')\nax.set_title('Hit@1 by Candidate Count')\nax.set_xticks(x)\nax.set_xticklabels(cand_stats['cand_bin'])\nfor i, row in cand_stats.iterrows():\n    ax.text(list(x)[list(cand_stats.index).index(i)], row['hit1'] * 100 + 1, f\"n={int(row['n'])}\", ha='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"GT vendor repeat count in history:\")\nprint(df['gt_vendor_in_history'].value_counts().sort_index())\nprint(f\"\\nCorrelation between vendor repeat count and Hit@1:\")\nprint(f\"  Mean rank when vendor seen 1x: {df[df['gt_vendor_in_history']==1]['ground_truth_rank'].mean():.2f}\")\nprint(f\"  Mean rank when vendor seen 2x: {df[df['gt_vendor_in_history']==2]['ground_truth_rank'].mean():.2f}\")\nprint(f\"  Mean rank when vendor seen 3+: {df[df['gt_vendor_in_history']>=3]['ground_truth_rank'].mean():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Failure Analysis\n\nDeep dive into cases where the pipeline fails (rank 0 or high rank)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.352098Z",
     "iopub.status.busy": "2026-01-27T09:02:51.352028Z",
     "iopub.status.idle": "2026-01-27T09:02:51.355508Z",
     "shell.execute_reply": "2026-01-27T09:02:51.355284Z"
    }
   },
   "outputs": [],
   "source": "# Failure analysis: rank 0 (not found) cases\nnot_found_df = df[df['ground_truth_rank'] == 0].copy()\n\nprint(f\"NOT FOUND cases (rank 0): {len(not_found_df)} ({len(not_found_df)/len(df)*100:.1f}%)\")\nprint(\"=\" * 60)\n\n# Why was GT not found?\n# 1. GT cuisine not predicted by Round 1\ncuisine_missed = not_found_df[~not_found_df['gt_cuisine_in_round1']]\n# 2. GT cuisine predicted but vendor not in candidates\ncuisine_hit_vendor_miss = not_found_df[not_found_df['gt_cuisine_in_round1'] & ~not_found_df['gt_in_candidates']]\n# 3. GT in candidates but ranked 0 (shouldn't happen normally)\nin_cands_but_0 = not_found_df[not_found_df['gt_in_candidates']]\n\nprint(f\"\\nBreakdown of NOT FOUND cases:\")\nprint(f\"  GT cuisine NOT predicted by R1:           {len(cuisine_missed)} ({len(cuisine_missed)/len(not_found_df)*100:.1f}%)\")\nprint(f\"  GT cuisine predicted, vendor NOT in cands: {len(cuisine_hit_vendor_miss)} ({len(cuisine_hit_vendor_miss)/len(not_found_df)*100:.1f}%)\")\nprint(f\"  GT in candidates but still rank 0:         {len(in_cands_but_0)} ({len(in_cands_but_0)/len(not_found_df)*100:.1f}%)\")\n\n# Cuisine distribution of not-found cases\nprint(f\"\\nCuisines most frequently not found:\")\nnf_cuisines = Counter(not_found_df['ground_truth_cuisine'])\nfor cuisine, count in nf_cuisines.most_common(10):\n    total_for_cuisine = (df['ground_truth_cuisine'] == cuisine).sum()\n    print(f\"  {cuisine}: {count}/{total_for_cuisine} not found ({count/total_for_cuisine*100:.0f}%)\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Detailed examples of failure cases\nprint(\"FAILURE EXAMPLES:\")\nprint(\"=\" * 80)\n\n# Example 1: Cuisine missed by Round 1\nif len(cuisine_missed) > 0:\n    row = cuisine_missed.iloc[0]\n    print(f\"\\n--- Type 1: GT Cuisine Not Predicted ---\")\n    print(f\"Sample {row['sample_idx']}  |  Customer: {row['customer_id']}\")\n    print(f\"  GT: {row['ground_truth']}  (cuisine: {row['ground_truth_cuisine']})\")\n    print(f\"  R1 predicted: {row['round1_predicted_cuisines']}\")\n    history_cuisines = [t[1] for t in row['order_history_tuples']]\n    print(f\"  History cuisines: {history_cuisines}\")\n    print(f\"  LightGCN top-5: {[(c, f'{s:.2f}') for c, s in row['lightgcn_top_cuisines'][:5]]}\")\n\n# Example 2: Cuisine hit but vendor not in candidates\nif len(cuisine_hit_vendor_miss) > 0:\n    row = cuisine_hit_vendor_miss.iloc[0]\n    print(f\"\\n--- Type 2: GT Cuisine Predicted, But Vendor Not Found ---\")\n    print(f\"Sample {row['sample_idx']}  |  Customer: {row['customer_id']}\")\n    print(f\"  GT: {row['ground_truth']}  (cuisine: {row['ground_truth_cuisine']})\")\n    print(f\"  R1 predicted: {row['round1_predicted_cuisines']}\")\n    same_cuisine_cands = [c for c in row['candidate_vendors'] if row['ground_truth_cuisine'] in c]\n    print(f\"  Candidates with same cuisine: {same_cuisine_cands[:5]}\")\n    print(f\"  Total candidates: {row['candidate_count']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.356331Z",
     "iopub.status.busy": "2026-01-27T09:02:51.356213Z",
     "iopub.status.idle": "2026-01-27T09:02:51.358809Z",
     "shell.execute_reply": "2026-01-27T09:02:51.358649Z"
    }
   },
   "outputs": [],
   "source": "# High rank (found but ranked poorly) analysis\nhigh_rank = df[(df['ground_truth_rank'] > 5) & (df['ground_truth_rank'] > 0)]\nprint(f\"Cases ranked > 5 (found but poorly ranked): {len(high_rank)}\")\nprint(\"=\" * 60)\n\nif len(high_rank) > 0:\n    print(f\"\\n  Mean rank: {high_rank['ground_truth_rank'].mean():.1f}\")\n    print(f\"  Mean candidate count: {high_rank['candidate_count'].mean():.1f}\")\n    print(f\"  GT cuisine in R1: {high_rank['gt_cuisine_in_round1'].mean()*100:.1f}%\")\n    \n    # Show examples\n    print(f\"\\nExamples (showing 3):\")\n    for _, row in high_rank.head(3).iterrows():\n        print(f\"\\n  Sample {row['sample_idx']}: rank={row['ground_truth_rank']}, \"\n              f\"candidates={row['candidate_count']}\")\n        print(f\"    GT: {row['ground_truth']}\")\n        print(f\"    Top-3 in final ranking: {row['final_ranking'][:3]}\")\n        print(f\"    GT vendor in history {row['gt_vendor_in_history']}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Similar Users Analysis\n\nHow do similar user signals affect prediction quality?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.359358Z",
     "iopub.status.busy": "2026-01-27T09:02:51.359301Z",
     "iopub.status.idle": "2026-01-27T09:02:51.361706Z",
     "shell.execute_reply": "2026-01-27T09:02:51.361543Z"
    }
   },
   "outputs": [],
   "source": "# Similar users analysis\ndf['num_similar_users'] = df['similar_users'].apply(len)\ndf['max_similarity'] = df['similar_users'].apply(\n    lambda users: max(u['similarity'] for u in users) if users else 0\n)\ndf['mean_similarity'] = df['similar_users'].apply(\n    lambda users: np.mean([u['similarity'] for u in users]) if users else 0\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Similarity score distribution\nax = axes[0]\nax.hist(df['max_similarity'], bins=30, color='#3498db', edgecolor='black', alpha=0.7)\nax.set_xlabel('Max Similarity Score')\nax.set_ylabel('Count')\nax.set_title('Distribution of Max Similarity Score')\nax.axvline(x=df['max_similarity'].median(), color='red', linestyle='--', label=f'Median={df[\"max_similarity\"].median():.4f}')\nax.legend()\n\n# Hit@1 by similarity quartile\ndf['sim_quartile'] = pd.qcut(df['max_similarity'], q=4, labels=['Q1 (low)', 'Q2', 'Q3', 'Q4 (high)'])\nsim_stats = df.groupby('sim_quartile', observed=True).agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean()),\n    found=('ground_truth_rank', lambda x: (x > 0).mean())\n).reset_index()\n\nax = axes[1]\nbars = ax.bar(range(len(sim_stats)), sim_stats['hit1'] * 100, color='#9b59b6', edgecolor='black', alpha=0.8)\nax.set_xlabel('Max Similarity Quartile')\nax.set_ylabel('Hit@1 (%)')\nax.set_title('Hit@1 by User Similarity Quartile')\nax.set_xticks(range(len(sim_stats)))\nax.set_xticklabels(sim_stats['sim_quartile'])\nfor bar, (_, row) in zip(bars, sim_stats.iterrows()):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f\"n={int(row['n'])}\", ha='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Similarity stats:\")\nprint(f\"  Mean max similarity: {df['max_similarity'].mean():.6f}\")\nprint(f\"  Similar users per case: {df['num_similar_users'].mean():.1f} (all have {df['num_similar_users'].unique()})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. LightGCN Cuisine Signal Quality\n\nHow well does LightGCN rank the ground truth cuisine? Does a higher LightGCN ranking for the cuisine translate to better final performance?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.362374Z",
     "iopub.status.busy": "2026-01-27T09:02:51.362314Z",
     "iopub.status.idle": "2026-01-27T09:02:51.364646Z",
     "shell.execute_reply": "2026-01-27T09:02:51.364515Z"
    }
   },
   "outputs": [],
   "source": "# LightGCN cuisine ranking quality\nlgcn_rank_stats = df.groupby('lightgcn_gt_cuisine_rank').agg(\n    n=('ground_truth_rank', 'size'),\n    hit1=('ground_truth_rank', lambda x: (x == 1).mean()),\n    hit3=('ground_truth_rank', lambda x: (x.between(1, 3)).mean()),\n    found=('ground_truth_rank', lambda x: (x > 0).mean()),\n    cuisine_in_r1=('gt_cuisine_in_round1', 'mean')\n).reset_index()\n\nprint(\"Performance by LightGCN Cuisine Rank (of GT cuisine):\")\nprint(\"=\" * 90)\nprint(f\"{'LightGCN Rank':>13}  {'N':>4}  {'Hit@1':>6}  {'Hit@3':>6}  {'Found%':>7}  {'R1 Predicts Cuisine%':>20}\")\nprint(\"-\" * 90)\nfor _, row in lgcn_rank_stats.iterrows():\n    rank_label = 'Not in top 10' if row['lightgcn_gt_cuisine_rank'] == 0 else int(row['lightgcn_gt_cuisine_rank'])\n    print(f\"{str(rank_label):>13}  {int(row['n']):>4}  {row['hit1']*100:>5.1f}%  {row['hit3']*100:>5.1f}%  {row['found']*100:>6.1f}%  {row['cuisine_in_r1']*100:>19.1f}%\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nvalid_ranks = lgcn_rank_stats[lgcn_rank_stats['lightgcn_gt_cuisine_rank'] > 0]\nax.bar(valid_ranks['lightgcn_gt_cuisine_rank'], valid_ranks['hit1'] * 100,\n       color='#2ecc71', edgecolor='black', alpha=0.8, label='Hit@1')\nax.plot(valid_ranks['lightgcn_gt_cuisine_rank'], valid_ranks['found'] * 100,\n        'o-', color='#e74c3c', label='Found Rate')\nax.set_xlabel('LightGCN Rank of GT Cuisine')\nax.set_ylabel('Rate (%)')\nax.set_title('Final Performance vs LightGCN Cuisine Ranking')\nax.legend()\nfor _, row in valid_ranks.iterrows():\n    ax.text(row['lightgcn_gt_cuisine_rank'], row['hit1'] * 100 + 2,\n            f\"n={int(row['n'])}\", ha='center', fontsize=7)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Round 2 Vendor Ranking Quality\n\nWhen GT is in candidates, how well does Round 2 rank it?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.365184Z",
     "iopub.status.busy": "2026-01-27T09:02:51.365127Z",
     "iopub.status.idle": "2026-01-27T09:02:51.367439Z",
     "shell.execute_reply": "2026-01-27T09:02:51.367308Z"
    }
   },
   "outputs": [],
   "source": "# Round 2 ranking quality (only when GT is in candidates)\ngt_in_found = gt_in.copy()\n\nprint(f\"When GT vendor IS in candidates ({len(gt_in_found)} cases):\")\nprint(\"=\" * 60)\n\nfor k in [1, 3, 5]:\n    hit = (gt_in_found['ground_truth_rank'].between(1, k)).sum() / len(gt_in_found)\n    print(f\"  Hit@{k}: {hit*100:.1f}%\")\n\nprint(f\"  Mean rank: {gt_in_found[gt_in_found['ground_truth_rank'] > 0]['ground_truth_rank'].mean():.2f}\")\n\n# How often does R2 put GT vendor at rank 1 when it's in candidates?\nr2_rank1_given_cand = (gt_in_found['ground_truth_rank'] == 1).sum()\nprint(f\"\\n  GT ranked #1 when in candidates: {r2_rank1_given_cand}/{len(gt_in_found)} ({r2_rank1_given_cand/len(gt_in_found)*100:.1f}%)\")\n\n# Rank distribution for cases where GT is in candidates\nfig, ax = plt.subplots(figsize=(8, 4))\nin_cand_ranks = gt_in_found['ground_truth_rank'].value_counts().sort_index()\ncolors = ['#e74c3c' if r == 0 else '#2ecc71' if r == 1 else '#3498db' for r in in_cand_ranks.index]\nax.bar(in_cand_ranks.index, in_cand_ranks.values, color=colors, edgecolor='black', alpha=0.8)\nax.set_xlabel('Ground Truth Rank')\nax.set_ylabel('Count')\nax.set_title(f'Rank Distribution When GT is in Candidates (n={len(gt_in_found)})')\nplt.tight_layout()\nplt.show()\n\n# Show examples where GT is in candidates but poorly ranked\npoorly_ranked = gt_in_found[gt_in_found['ground_truth_rank'] > 3]\nif len(poorly_ranked) > 0:\n    print(f\"\\nExamples where GT is in candidates but ranked > 3 ({len(poorly_ranked)} cases):\")\n    for _, row in poorly_ranked.head(3).iterrows():\n        print(f\"\\n  Sample {row['sample_idx']}: rank={row['ground_truth_rank']}, candidates={row['candidate_count']}\")\n        print(f\"    GT: {row['ground_truth']}\")\n        print(f\"    Top-3 ranked: {row['final_ranking'][:3]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Pipeline Bottleneck Analysis\n\nWhere does the pipeline lose the most? Break down the failure into pipeline stages."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.367926Z",
     "iopub.status.busy": "2026-01-27T09:02:51.367866Z",
     "iopub.status.idle": "2026-01-27T09:02:51.369872Z",
     "shell.execute_reply": "2026-01-27T09:02:51.369744Z"
    }
   },
   "outputs": [],
   "source": "# Pipeline bottleneck funnel\ntotal = len(df)\n\n# Stage 1: LightGCN cuisine ranking - is GT cuisine in top 10?\nlgcn_top10 = (df['lightgcn_gt_cuisine_rank'] > 0).sum()\nlgcn_top3 = (df['lightgcn_gt_cuisine_rank'].between(1, 3)).sum()\nlgcn_top1 = (df['lightgcn_gt_cuisine_rank'] == 1).sum()\n\n# Stage 2: Round 1 predicts GT cuisine\nr1_correct = df['gt_cuisine_in_round1'].sum()\n\n# Stage 3: GT vendor in candidates\ngt_cand = df['gt_in_candidates'].sum()\n\n# Stage 4: GT vendor found in final ranking\ngt_found = (df['ground_truth_rank'] > 0).sum()\n\n# Stage 5: GT vendor at rank 1\ngt_rank1 = (df['ground_truth_rank'] == 1).sum()\n\nstages = [\n    ('Total samples', total),\n    ('LightGCN: GT cuisine in top-10', lgcn_top10),\n    ('LightGCN: GT cuisine in top-3', lgcn_top3),\n    ('Round 1: GT cuisine predicted', r1_correct),\n    ('GT vendor in candidates', gt_cand),\n    ('GT vendor found in ranking', gt_found),\n    ('GT vendor at rank 1', gt_rank1),\n]\n\nprint(\"PIPELINE FUNNEL:\")\nprint(\"=\" * 70)\nfor stage, count in stages:\n    lost = total - count\n    print(f\"  {stage:<40} {count:>4}/{total}  ({count/total*100:>5.1f}%)  lost: {lost}\")\n\n# Visualize funnel\nfig, ax = plt.subplots(figsize=(10, 6))\nlabels = [s[0] for s in stages]\nvalues = [s[1] for s in stages]\ncolors_funnel = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(stages)))\n\nbars = ax.barh(range(len(stages)-1, -1, -1), values, color=colors_funnel)\nax.set_yticks(range(len(stages)-1, -1, -1))\nax.set_yticklabels(labels)\nax.set_xlabel('Number of Cases')\nax.set_title('Pipeline Funnel: Where Cases Are Lost')\n\nfor bar, val in zip(bars, values):\n    ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n            f'{val} ({val/total*100:.1f}%)', va='center')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.370388Z",
     "iopub.status.busy": "2026-01-27T09:02:51.370291Z",
     "iopub.status.idle": "2026-01-27T09:02:51.372380Z",
     "shell.execute_reply": "2026-01-27T09:02:51.372252Z"
    }
   },
   "outputs": [],
   "source": "## 12. Summary & Key Findings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T09:02:51.372829Z",
     "iopub.status.busy": "2026-01-27T09:02:51.372776Z",
     "iopub.status.idle": "2026-01-27T09:02:51.374695Z",
     "shell.execute_reply": "2026-01-27T09:02:51.374568Z"
    }
   },
   "outputs": [],
   "source": "# Summary\ntotal = len(df)\nfound_count = (df['ground_truth_rank'] > 0).sum()\nhit1 = (df['ground_truth_rank'] == 1).sum()\nhit3 = (df['ground_truth_rank'].between(1, 3)).sum()\nhit5 = (df['ground_truth_rank'].between(1, 5)).sum()\n\nsummary = f\"\"\"\n# Stage 9 Repeat Evaluation Summary\n\n## Dataset\n- Total samples: {total}\n- Unique customers: {df['customer_id'].nunique()}\n- Ground truth cuisines: {df['ground_truth_cuisine'].nunique()}\n\n## Overall Performance\n| Metric | Value |\n|--------|-------|\n| Hit@1 | {hit1/total*100:.1f}% ({hit1}/{total}) |\n| Hit@3 | {hit3/total*100:.1f}% ({hit3}/{total}) |\n| Hit@5 | {hit5/total*100:.1f}% ({hit5}/{total}) |\n| MRR | {df['mrr'].mean():.4f} |\n| GT Found Rate | {found_count/total*100:.1f}% ({found_count}/{total}) |\n| Avg Rank (when found) | {found['ground_truth_rank'].mean():.2f} |\n\n## Pipeline Breakdown\n| Stage | Rate |\n|-------|------|\n| LightGCN: GT cuisine in top-3 | {(df['lightgcn_gt_cuisine_rank'].between(1,3)).mean()*100:.1f}% |\n| Round 1: GT cuisine predicted | {df['gt_cuisine_in_round1'].mean()*100:.1f}% |\n| GT vendor in candidates | {df['gt_in_candidates'].mean()*100:.1f}% |\n| GT vendor found in ranking | {(df['ground_truth_rank'] > 0).mean()*100:.1f}% |\n\n## Key Findings\n1. **Candidate generation is the bottleneck**: {(~df['gt_in_candidates']).sum()} cases ({(~df['gt_in_candidates']).mean()*100:.1f}%) fail because the GT vendor isn't in the candidate list\n2. **Round 1 cuisine prediction is strong**: {df['gt_cuisine_in_round1'].mean()*100:.1f}% accuracy on predicting the GT cuisine\n3. **When GT is in candidates, ranking works well**: Hit@1 = {(gt_in['ground_truth_rank']==1).mean()*100:.1f}% for cases with GT in candidates\n4. **Top cuisines**: pizza ({(df['ground_truth_cuisine']=='pizza').sum()}), burgare ({(df['ground_truth_cuisine']=='burgare').sum()}), asiatiskt ({(df['ground_truth_cuisine']=='asiatiskt').sum()}) are most common GT cuisines\n\"\"\"\n\ndisplay(Markdown(summary))"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Final visualization: overall metrics\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Hit@K curve\nax = axes[0]\nks = range(1, 21)\nhit_all = [(df['ground_truth_rank'].between(1, k)).mean() * 100 for k in ks]\nhit_in_cand = [(gt_in['ground_truth_rank'].between(1, k)).mean() * 100 for k in ks]\n\nax.plot(ks, hit_all, 'o-', label='All samples', linewidth=2, markersize=6)\nax.plot(ks, hit_in_cand, 's-', label='GT in candidates only', linewidth=2, markersize=6)\nax.set_xlabel('K')\nax.set_ylabel('Hit@K (%)')\nax.set_title('Hit@K Curves')\nax.legend()\nax.set_xticks([1, 3, 5, 10, 15, 20])\nax.grid(True, alpha=0.3)\n\n# Right: Failure breakdown pie\nax = axes[1]\nr1_miss = (~df['gt_cuisine_in_round1']).sum()\nr1_hit_cand_miss = (df['gt_cuisine_in_round1'] & ~df['gt_in_candidates']).sum()\ncand_hit_rank_miss = (df['gt_in_candidates'] & (df['ground_truth_rank'] > 1)).sum()\nsuccess = (df['ground_truth_rank'] == 1).sum()\n\nslices = [success, cand_hit_rank_miss, r1_hit_cand_miss, r1_miss]\nlabels = [\n    f'Hit@1 ({success})',\n    f'In cands, rank>1 ({cand_hit_rank_miss})',\n    f'Cuisine OK, vendor miss ({r1_hit_cand_miss})',\n    f'Cuisine missed by R1 ({r1_miss})'\n]\ncolors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\n\nax.pie(slices, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\nax.set_title('Outcome Breakdown')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 60)\nprint(\"Analysis complete. Use lookup(idx) to inspect individual cases.\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}