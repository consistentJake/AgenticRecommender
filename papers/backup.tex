\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{parskip}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Improving Ranking through Reasoning: A Multi-Step Agentic Framework for Recommender Systems
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}

}

\maketitle

\begin{abstract}
Traditional recommender systems, primarily driven by Deep Learning and ID-based collaborative filtering, excel at capturing latent user preferences but often struggle with data sparsity and lack the semantic reasoning capabilities required for complex, intent-driven tasks. While the integration of Large Language Models (LLMs) has introduced rich semantic understanding to the field, existing approaches predominantly utilize LLMs as static scoring functions or passive conversationalists, failing to fully leverage their potential for deliberate planning and problem-solving. In this paper, we propose a novel Agentic Recommender System, a framework that shifts the ranking paradigm from simple pattern matching to autonomous, multi-step reasoning.

Our approach designs a comprehensive agentic workflow that empowers the recommender to decompose complex user queries, utilize external tools, and engage in iterative self-correction. Through extensive experiments, we demonstrate that this framework significantly outperforms state-of-the-art graph-based models. Furthermore, our analysis reveals pivotal insights regarding model optimization: we find that inference-time scaling—allocating compute to "System 2" reasoning at test time—yields substantial performance gains by enabling the model to refine its logic, whereas Supervised Fine-Tuning (SFT) offers diminishing returns compared to the inherent reasoning capabilities of frontier models. By achieving superior ranking performance with minimal training overhead, our work positions agentic workflows as a foundational and efficient path toward the next generation of intent-aware recommender systems.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}

\section{Introduction}
In the era of information overload, Recommender Systems (RS) have become the fundamental engine powering user experience across the digital landscape. From e-commerce platforms like Amazon to content streaming services like Netflix and YouTube, these systems serve as the primary filter between users and vast catalogs of items, driving both user satisfaction and business revenue \cite{covington2016deep}. As user needs evolve from simple item retrieval to complex, intent-driven decision-making, the architectural paradigms of these systems have undergone significant transformations.

For the past decade, Deep Learning (DL) has dominated the field. Models such as Neural Collaborative Filtering (NCF) \cite{he2017neural} and Deep Interest Networks (DIN) \cite{zhou2018deep} moved beyond the linearity of Matrix Factorization, enabling the capture of intricate, non-linear user-item interactions. Furthermore, the introduction of sequential modeling—exemplified by SASRec \cite{kang2018self} and BERT4Rec \cite{sun2019bert4rec}—allowed systems to view user preference not as a static profile, but as a dynamic trajectory, significantly improving next-item prediction accuracy. However, despite their success, these classical DL models rely heavily on ID-based collaborative signals, often treating items as abstract indices and failing to comprehend the rich semantic context or the underlying reasoning behind a user's choice.

The advent of Large Language Models (LLMs) has begun to address this semantic gap. By leveraging extensive world knowledge and superior natural language understanding, LLM-based recommenders (e.g., MMREC \cite{tian2024mmrec}, TALLRec \cite{bao2023tallrec}) have demonstrated remarkable zero-shot capabilities and the ability to interpret textual item descriptions. Yet, a critical limitation remains: current LLM-based approaches predominantly operate as static predictors or passive conversationalists. They struggle with multi-step reasoning, long-horizon planning, and proactive tool usage—capabilities essential for handling complex user goals that require more than a single-turn retrieval. For instance, planning a travel itinerary requires not just suggesting a hotel, but coordinating location, budget constraints, and activities—a task that demands an agentic workflow rather than simple probability ranking.

To bridge this gap, we observe a rising trend in Agentic AI—autonomous systems capable of perception, reasoning, and action execution \cite{wang2024macrec, li2024recommender}. In this work, we propose a novel framework that integrates Agentic AI into the recommendation pipeline. Unlike traditional models that map inputs directly to outputs, our solution designs an agentic flow where the recommender acts as an intelligent agent. This agent decomposes complex user queries into sub-tasks, utilizes external tools (such as search engines or domain-specific APIs), and reasons iteratively to refine its outputs. By empowering the recommender with the agency to "think" before it "acts," we aim to close the reasoning gap present in existing literature, delivering recommendations that are not only accurate but also actionable and contextually grounded.

\section{Related Work}\label{AA}
Our work builds upon three distinct pillars of research: classical Deep Learning for recommendation, the application of LLMs in RS, and the emerging field of Agentic AI.

\subsection{Classical Deep Learning for Recommendation}

The shift from traditional Matrix Factorization to neural architectures marked a pivotal moment in RS research. NCF \cite{he2017neural} generalized matrix factorization by replacing the inner product with a multi-layer perceptron to learn non-linear user-item interactions. To address the dynamic nature of user preferences, sequential recommendation models emerged. SASRec \cite{kang2018self} and BERT4Rec \cite{sun2019bert4rec} adapted the Transformer architecture to capture long-term dependencies in user behavior sequences via self-attention mechanisms. Furthermore, to better utilize rich feature sets in industrial settings, models like the DIN \cite{zhou2018deep} introduced attention mechanisms to learn the relevance of historical behaviors to a target item adaptively. While these models excel at matching efficiency and capturing collaborative signals, they fundamentally lack semantic reasoning capabilities, treating items primarily as ID embeddings.

\subsection{Large Language Models for Recommendation}

The emergence of LLMs has catalyzed a paradigm shift towards "Recommendation as Language Processing." Early works focused on using BERT-based encoders to enhance content representations. More recently, generative approaches have gained traction. TALLRec \cite{bao2023tallrec} demonstrated that instruction tuning could significantly align general-purpose LLMs with recommendation objectives, even with limited training data. Other works, such as Chat-REC \cite{gao2023chat}, leverage the conversational abilities of LLMs to build interactive recommenders that can refine user preferences through dialogue. LLM has also been proven to be effective in the recommendation system while handling multi-modal as well as missing data \cite{tian2024mmrec, ding2024data}
despite these advancements, these models largely function as "System 1" thinkers—relying on pattern matching and immediate generation—without the "System 2" capabilities of deliberate reasoning and planning.

\subsection{Agentic AI in Recommendation}

Agentic AI represents the next frontier, moving from passive prediction to active simulation and problem-solving. Early exploration in this domain focused on using agents to simulate user behavior. Agent4Rec \cite{zhang2024generative} utilize LLM-based generative agents to simulate users in a sandbox environment, observing emergent social behaviors and validating recommendation algorithms without live traffic. More recently, research has pivoted toward using agents as the recommender itself. Research has pivoted toward designing agents that actively construct recommendations through collaboration and specialized reasoning. A prime example is MACRec \cite{wang2024macrec}, which proposes a multi-agent collaboration framework where distinct agents—acting as managers, searchers, and analysts—cooperate to decompose and solve complex recommendation tasks. This collaborative approach allows for a more holistic handling of user needs compared to monolithic models.
This trend towards specialization is also evident in domain-specific applications, where the complexity of user intent requires distinct handling mechanisms. For instance, in food delivery scenarios, it was demonstrate that user intent often bifurcates between "repeat" consumption and "exploration" of new items \cite{li2024recommender}. They propose a purpose-driven framework where specialized modules (functionally analogous to agents) are dedicated to either repeat or exploration tasks, ensuring that the system can dynamically adapt to the user's immediate goal. Our work extends these directions by designing a comprehensive agentic flow that not only employs tool usage but also integrates this type of intent-aware reasoning and multi-step planning to handle complex, goal-oriented recommendation tasks.


\section{Agentic Recommender Design}

We frame repeat-order prediction as a multi-agent workflow comprising these specialized roles:
a \textbf{Manager} that orchestrates the end-to-end pipeline,
a \textbf{Profiler} that retrieves and structures evidence from heterogeneous data sources, build profiles for users and vendors(restaurants)
\textbf{Similarity Agent} introduce collobrative signal into context, using  traditional recommendation methods to provide initial vendor ranking recommendation.
\textbf{Similarity Agent} provide baisc user-item similarity to help Analyzer item ranking in Round1.
an \textbf{Analyzer} that performs coarse-grained cuisine reasoning in Round~1,
and a \textbf{Critic} that conducts fine-grained vendor ranking in Round~2.
The Manager drives a deterministic two-round pipeline: it first dispatches the Profiler to assemble user history, collaborative filtering signals, and geospatial indices; then invokes the Analyzer to narrow the search space to a small set of candidate cuisines; and finally activates the Critic to produce the final vendor ranking by integrating collaborative evidence from similar users.
%% Manager: orchestration, for first Round1, gather user profiler and similarity agent information and send to analyzer
%% User Profiler: record user historical orders, store in (vendor, primary_cuisine, order time) tuples. Support similar user retrieval
%% Restaurant Profiler: primary cuisine, build mapping between geo location and vendor list. Support quick
%% Similarity Agent: provide baisc user-item similarity to help Analyer item ranking in Round1. 
%% Analyzer: Round1 Agent, utilize user history and similarity agent input, and the order time, predict top candiate cuisines
%% Manager: gather Round1 candiate cuisines, based on order's geo location, find the top restaurants falling in the candidate cuisines list, use User profiler, find top 5 similar users, gather thier historic orders, added into context for critic.
%% Critic: Based on what manager provide, generate final ranking result
All inter-agent communication follows a hub-and-spoke pattern through the Manager, enforcing strict role specialization and reproducibility.

Before any LLM reasoning occurs, the Profiler gathers structured evidence from three subsystems:
\begin{itemize}
    \item \textbf{User Profiler} Constructs chronological purchase histories, extracts behavioral features (cuisine frequency, temporal ordering patterns, vendor loyalty), and computes Swing-based user--user similarities.
    It retrieves up to five top neighbors and curates a maximum of twenty relevant peer orders---filtered by predicted cuisines---for injection into Round~2 prompts.
    \item \textbf{Vendor Profiler} Maintains a geohash\,$\rightarrow$\,cuisine\,$\rightarrow$\,vendor index that maps delivery areas to available vendors per cuisine type, enabling geospatially constrained candidate retrieval once Round~1 predicts the target cuisines.
    \item \textbf{Similarity Agent} Trains a three-layer LightGCN \cite{he2020lightgcn} with 64-dimensional embeddings optimized with Bayesian Personalized Ranking (BPR) loss on the user--cuisine bipartite graph, delivering embedding-derived cuisine affinity scores that seed the Analyzer's reasoning in Round~1.
\end{itemize}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{food_delivery.png}
    \caption{Agentic Workflow}
    \label{fig:graph1}
\end{figure*}
%% ---- Round 1: Analyzer ----

\paragraph{Round~1: The Analyzer.}
The Analyzer agent receives a prompt composed of the following inputs:
\begin{itemize}
    \item The user's \textbf{complete order history}, formatted as time-stamped entries \texttt{vendor\_id||cuisine (DayName Hour:00)}, ordered oldest to newest.
    \item The \textbf{target prediction context}: day-of-week and hour (e.g., ``Wednesday at 18:00'').
    \item \textbf{LightGCN collaborative filtering scores}: the top-10 cuisines ranked by user--cuisine embedding dot-product similarity, each annotated with a numerical affinity score indicating popularity among behaviorally similar users.
\end{itemize}
The prompt instructs the Analyzer to reason along four dimensions:
(i)~\emph{temporal patterns}---day-of-week and meal-time preferences,
(ii)~\emph{cuisine frequency}---cuisines ordered more often are more likely,
(iii)~\emph{collaborative filtering scores}---higher scores indicate cuisines popular among similar users, and
(iv)~\emph{recency}---recent cuisine choices may reflect evolving preferences.
The Analyzer outputs a JSON-formatted top-three cuisine list together with a brief rationale.
If JSON parsing fails, the pipeline falls back to the pure LightGCN score ranking, ensuring robustness.

Figure~\ref{fig:round1_prompt} shows a representative Round~1 prompt.

\begin{figure}[t]
\centering
\small
\begin{tcolorbox}[colback=gray!5, colframe=gray!60, title=Round~1 Prompt (Analyzer), fonttitle=\bfseries\small, left=2mm, right=2mm, top=1mm, bottom=1mm]
\ttfamily\scriptsize
You are a food delivery recommendation system.\\
Based on this user's order history, predict the\\
top 3 most likely PRIMARY CUISINES for their\\
next order.\\[4pt]
\#\# Order History (15 orders, oldest to newest):\\
Each entry: vendor\_id||cuisine (day\_of\_week time)\\
1. V23||Thai (Mon 19:00)\\
2. V45||Chinese (Wed 12:00)\\
\ldots\\
15. V23||Thai (Sat 19:00)\\[4pt]
\#\# Predict for: Wednesday at 18:00\\[4pt]
\#\# Collaborative Filtering Scores:\\
1. Thai: 0.847\quad 2. Chinese: 0.723\quad 3. Japanese: 0.681\\
\ldots\\[4pt]
Consider:\\
- Temporal patterns: day-of-week, meal-time\\
- Cuisine frequency: more ordered = more likely\\
- CF scores: popular among similar users\\
- Recency: recent choices reflect current prefs\\[4pt]
Return exactly 3 cuisines as JSON:\\
\{"cuisines": [...], "reasoning": "..."\}
\end{tcolorbox}
\caption{Illustrative Round~1 prompt.  The Analyzer receives the user's full order history, the target prediction time, and LightGCN cuisine affinity scores.}
\label{fig:round1_prompt}
\end{figure}

%% ---- Round 2: Critic ----

\paragraph{Round~2 Input Construction.}
Once the Analyzer narrows the cuisine space, the Manager orchestrates candidate construction for the Critic.
The central design choice in Round~2 is the injection of \textbf{collaborative filtering evidence from similar users} to guide vendor ranking.
Many state-of-the-art deep learning methods for food delivery recommendation---including our baseline DPVP \cite{zhang2023dpvp}, which employs time-aware graphs and subgraphs---require end-to-end training with extensive hyperparameter tuning and yield representations that are difficult to inspect or explain.
Instead, we adopt \textbf{Swing similarity} \cite{yang2020swing}, a lightweight user--user measure from Alibaba that computes similarity through co-purchase patterns while penalizing noisy signals from overly active users and overly popular items:
\begin{equation}
\mathrm{sim}(u_1, u_2) = \!\!\sum_{i \,\in\, I_{u_1} \cap I_{u_2}} \frac{1}{\bigl(|I_{u_1}| + \alpha_1\bigr)^{\!\beta} \cdot \bigl(|I_{u_2}| + \alpha_1\bigr)^{\!\beta} \cdot \bigl(|U_i| + \alpha_2\bigr)}
\label{eq:swing}
\end{equation}
where $|I_u|$ is the number of items user~$u$ has interacted with, $|U_i|$ is the number of users who ordered item~$i$, and $\alpha_1{=}5.0$, $\alpha_2{=}1.0$, $\beta{=}0.3$ are smoothing parameters.
The anti-noise property ensures that two users sharing a rare vendor receive a much higher similarity boost than two users sharing a highly popular one, producing more meaningful neighbor evidence than na\"{\i}ve co-occurrence counting.

For each target user, the Profiler retrieves the \textbf{top-5 most similar users} via Swing, collects their training order records, and filters them to retain only orders whose cuisine matches one of the Round~1 predicted cuisines, capped at five records per neighbor and twenty records total.
This cuisine-filtered design prevents the Critic from being overwhelmed by irrelevant evidence and ensures that the collaborative signal is precisely aligned with the Analyzer's coarse cuisine forecast.

\paragraph{Round~2: The Critic.}
The Critic agent receives a prompt containing the following inputs:
\begin{itemize}
    \item The user's \textbf{recent order history} (last 10 orders), formatted as \texttt{vendor\_id||cuisine (DayName Hour:00)}.
    \item The \textbf{target prediction context}: day-of-week and hour.
    \item The \textbf{Round~1 predicted cuisines} from the Analyzer (e.g., ``Thai, Chinese, Japanese'').
    \item \textbf{Candidate vendors to rank}: up to 20 vendors retrieved from the geohash index conditioned on the Round~1 cuisines, \emph{shuffled} to mitigate position bias. Each candidate is formatted as \texttt{vendor\_id||cuisine}.
    \item \textbf{Similar users' recent orders}: cuisine-filtered records from Swing neighbors, each annotated with the similarity score (e.g., \texttt{V45||Chinese (Tue 12:00) [sim=0.123]}).
\end{itemize}
The Critic is instructed to reason about four factors: (i)~\emph{vendor loyalty}---vendors the user has ordered from before, (ii)~\emph{Round~1 cuisine predictions}---vendors matching top cuisines should rank higher, (iii)~\emph{temporal patterns}---day-of-week and meal-time preferences, and (iv)~\emph{similar users' vendor choices}---vendors frequently chosen by behavioral neighbors.
It produces a complete vendor ranking accompanied by a reflective rationale.
The pipeline validates JSON output, injects any missing vendors to ensure completeness, and degrades gracefully by extracting vendor mentions from free text when structured parsing fails.
Candidate shuffling across invocations ensures that any systematic position bias is averaged out.

Figure~\ref{fig:round2_prompt} shows a representative Round~2 prompt.

\begin{figure}[t]
\centering
\small
\begin{tcolorbox}[colback=gray!5, colframe=gray!60, title=Round~2 Prompt (Critic), fonttitle=\bfseries\small, left=2mm, right=2mm, top=1mm, bottom=1mm]
\ttfamily\scriptsize
You are a food delivery recommendation system.\\
Rank the candidate vendors from most to least\\
likely for this user's next order.\\[4pt]
\#\# User's Recent Order History (last 10 orders):\\
1. V23||Thai (Mon 19:00) \ldots\  10. V45||Chinese (Wed 12:00)\\[4pt]
\#\# Predict for: Wednesday at 18:00\\[4pt]
\#\# Round 1 Predicted Cuisines:\\
Thai, Chinese, Japanese\\[4pt]
\#\# Candidate Vendors to Rank:\\
1. V67||Thai\quad 2. V23||Thai\quad \ldots\quad 18. V89||Japanese\\[4pt]
\#\# Similar Users' Recent Orders (CF):\\
~~- V23||Thai (Tue 19:00) [sim=0.342]\\
~~- V45||Chinese (Wed 12:00) [sim=0.342]\\
~~- V67||Thai (Mon 18:00) [sim=0.287] \ldots\\[4pt]
Consider:\\
- Vendor loyalty, Round 1 cuisine predictions\\
- Temporal patterns, Similar users' choices\\
- Rank ALL 18 candidates\\[4pt]
Return JSON:\\
\{"final\_ranking": [...], "reflection": "..."\}
\end{tcolorbox}
\caption{Illustrative Round~2 prompt.  The Critic receives the user's recent history, the Analyzer's cuisine predictions, geohash-filtered candidate vendors (shuffled), and Swing-derived similar users' orders with similarity scores.}
\label{fig:round2_prompt}
\end{figure}

%% ---- Pipeline Flow ----

\paragraph{Pipeline Flow.}
The full collaboration flow is:
Manager $\rightarrow$ Profiler (user history, LightGCN scores, geohash index, Swing neighbors)
$\rightarrow$ Analyzer (Round~1 cuisine prediction)
$\rightarrow$ Profiler (geohash candidate retrieval, similar-user record filtering by predicted cuisines)
$\rightarrow$ Critic (Round~2 vendor ranking)
$\rightarrow$ Manager (output).
The two-round design deliberately narrows the search space before ranking: Round~1 reduces thousands of possible vendors to tens of candidates within the predicted cuisine categories and delivery area, and Round~2 then leverages collaborative evidence to discriminate among those candidates.
Hybrid intelligence emerges from the interplay of LightGCN embeddings (global collaborative structure), Swing user--user similarities (local peer behavior), geohash-constrained retrieval (spatial feasibility), and LLM reasoning (temporal and sequential pattern understanding), while explicit fallbacks at every stage---CF-only predictions, history-based ranking, free-text extraction---ensure graceful degradation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental Setup}

\textbf{Datasets}: We evaluate on two real-world food delivery datasets from the Delivery Hero platform~\cite{deliveryhero2023}: \textbf{DHRD-SE} (Stockholm, Sweden) and \textbf{DHRD-SG} (Singapore).
Table~\ref{tab:dataset_stats} summarizes their statistics.
Both datasets contain customer, vendor, product, cuisine, timestamp, geohash, and price fields.
A chronological split allocates earlier orders to training and later ones to testing.
We focus on repeat-order behavior by selecting test interactions whose user--vendor pair appeared in training and by filtering out users with fewer than five unique historical orders.

\begin{table}[t]
    \centering
    \caption{Dataset statistics. Rep.\% denotes the share of repeat orders (user--vendor pairs seen in training).}
    \label{tab:dataset_stats}
    \begin{tabular}{lrrrrr}
        \hline
        Dataset & \#Users & \#Vendors & \#Orders & \#Cuisines & Rep.\% \\
        \hline
        DHRD-SE & 7{,}900 & 1{,}131 & 128{,}329 & 39 & 56.5\% \\
        DHRD-SG & 46{,}129 & 7{,}296 & 791{,}088 & 78 & 42.3\% \\
        \hline
    \end{tabular}
\end{table}

Evaluation Metrics: We chose \textbf{HR@3} (Hit Rate at 3) and \textbf{NDCG@3} as our primary metrics, as they are the standard metrics for food delivery recommendation evaluation. We additionally report HR@1, HR@5, and MRR for completeness.

\textbf{Baseline}: Our primary baseline is \textbf{DPVP}~\cite{zhang2023dpvp}, a state-of-the-art food delivery recommender that constructs time-aware graphs and subgraphs to model users' dual period-varying preferences for locations, stores, and food.
DPVP represents the class of domain-specific deep learning models that require end-to-end training with careful hyperparameter tuning.
We additionally include results from the strongest reported baselines including RepRec (the current SOTA for repeat orders), DIN, SDIM, and other methods---as reference points.

% \textbf{Evaluation protocol differences.}
% Li et al.\ evaluate repeat recommendation by using \emph{all historically interacted stores} as the candidate set, meaning the ground-truth vendor is always present among candidates.
% Our pipeline instead uses \emph{geohash-filtered} candidates conditioned on the Analyzer's cuisine predictions, producing an average of 16--18 candidate vendors per query.
% The ground-truth vendor is present in our candidate set for 72.4\% of DHRD-SE samples and 57.8\% of DHRD-SG samples.
% When the ground truth is absent from the candidate set, the sample receives zero credit across all metrics.
% This makes our evaluation strictly harder in terms of candidate coverage, but more realistic in reflecting delivery-feasible recommendations.

\paragraph{LLM Configuration.}
We select open-source LLMs from the \textbf{Qwen3} family~\cite{qwen3} as our reasoning backbone for two reasons:
(1)~open-source models enable reproducibility, local deployment, and fine-tuning without proprietary API dependency, and
(2)~the Qwen3 series supports \emph{extended thinking mode}, which allows the model to perform internal chain-of-thought reasoning before producing structured output---a capability we hypothesize is essential for the multi-factor reasoning required in recommendation.
Specifically, we use \textbf{Qwen3-32B} (a 32-billion-parameter dense model) for DHRD-SE and \textbf{Qwen3-30B-A3B} (a mixture-of-experts variant with 30B total and 3B active parameters, with built-in thinking mode) for DHRD-SG.
Both models run with extended thinking enabled.
Round~1 uses temperature $0.3$ with a 4{,}096-token budget; Round~2 uses temperature $0.2$ with the same budget.

For the multi-model comparison study (Section~\ref{sec:model_comparison}), we additionally evaluate Google's Gemini Flash model family across three generations to examine the relationship between model reasoning capability and recommendation accuracy.

\paragraph{Implementation Details.}
LightGCN is implemented in PyTorch with 64-dimensional embeddings, three propagation layers, BPR loss, 50 epochs, learning rate $10^{-3}$, and L2 regularization $10^{-4}$.
Swing uses smoothing parameters $\alpha_1{=}5.0$, $\alpha_2{=}1.0$, $\beta{=}0.3$.
Candidate vendors are drawn from the geohash index conditioned on Round~1 cuisines, shuffled, and capped at 20.
Round~2 prompts include up to five Swing-similar users with at most 20 total records filtered to the predicted cuisines.
The evaluation pipeline issues up to 25 concurrent asynchronous requests with exponential-backoff retry (up to 10 attempts) and disk-caches LightGCN embeddings, Swing scores, and geohash indices for reproducibility.

%% ---- Overall Performance ----

\subsection{Overall Performance}

\paragraph{}
Table~\ref{tab:overall_performance} presents repeat-order ranking accuracy on both DHRD-SE and DHRD-SG.
On DHRD-SE, our best configuration (Gemini-3-Flash) achieves HR@3 of $0.660$ and NDCG@3 of $0.606$.
The NDCG@3 of $0.606$ exceeds DPVP's reported $0.550$ by a substantial margin ($+10.2\%$ relative), despite our pipeline requiring \emph{no task-specific training}.
The HR@3 gap to DPVP ($0.660$ vs.\ $0.677$) is only $2.5\%$, and much of this gap is attributable to the geohash filtering bottleneck: the ground-truth vendor appears in our candidate set for only 72.4\% of test samples.
When conditioning on samples where the ground truth \emph{is} present among candidates, the effective HR@3 rises to approximately $0.91$, indicating that the LLM's vendor ranking is highly accurate once the correct cuisine and delivery area are identified.

On DHRD-SG, which is a substantially larger and more diverse dataset (7$\times$ more vendors, 2$\times$ more cuisines), our pipeline with Qwen3-30B-A3B and thinking mode achieves HR@3 of $0.493$ and NDCG@3 of $0.468$.
The lower performance relative to DHRD-SE reflects the sparser user--vendor interaction density and wider cuisine diversity in Singapore, as well as the lower ground-truth candidate coverage (57.8\%).

\begin{table}[t]
    \centering
    \caption{Repeat-order performance. $\dagger$\,Results from Li et al.~\cite{li2024recommender} using all historically interacted stores as candidates. Our pipeline uses geohash-filtered candidates (avg.\ 16--18 vendors; GT present in 72.4\% of SE, 57.8\% of SG samples).}
    \label{tab:overall_performance}
    \begin{tabular}{llcc}
        \hline
        & Method & HR@3 & NDCG@3 \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\small DHRD-SE}}
        & DPVP$^\dagger$ & 0.677 & 0.550 \\
        & DIN$^\dagger$ & 0.763 & 0.623 \\
        & SDIM$^\dagger$ & 0.771 & 0.627 \\
        & RepRec$^\dagger$ (SOTA) & \textbf{0.783} & \textbf{0.659} \\
        \cmidrule{2-4}
        & Ours (Gemini-2.0-Flash) & 0.606 & 0.561 \\
        & Ours (Gemini-2.5-Flash) & 0.640 & 0.584 \\
        & Ours (Gemini-3-Flash) & 0.660 & 0.606 \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\small DHRD-SG}}
        & DPVP$^\dagger$ & 0.681 & 0.560 \\
        & DIN$^\dagger$ & 0.682 & 0.563 \\
        & RepRec$^\dagger$ (SOTA) & \textbf{0.709} & \textbf{0.593} \\
        \cmidrule{2-4}
        & Ours (Qwen3-30B-A3B, no think) & 0.451 & 0.432 \\
        & Ours (Qwen3-30B-A3B, think) & 0.493 & 0.468 \\
        \hline
    \end{tabular}
\end{table}

%% ---- Multi-Model Comparison ----

\subsection{Multi-Model Comparison}
\label{sec:model_comparison}

\paragraph{}
To investigate how LLM reasoning capability affects recommendation quality, we evaluate multiple models on the same DHRD-SE repeat-order task with deterministic user ordering.
Table~\ref{tab:model_comparison} reports results across three generations of Google Gemini Flash models and the Qwen3 thinking mode ablation on DHRD-SG.

\begin{table}[t]
    \centering
    \caption{Multi-model comparison on repeat-order prediction. All models are evaluated on the full test set of each dataset. DHRD-SE results use Gemini Flash variants; DHRD-SG results compare thinking mode on/off for Qwen3-30B-A3B.}
    \label{tab:model_comparison}
    \begin{tabular}{llcccc}
        \hline
        Dataset & Model & HR@1 & HR@3 & NDCG@3 & MRR \\
        \hline
        \multirow{3}{*}{D-SE}
        & Gemini-2.0-Flash & 0.494 & 0.606 & 0.561 & 0.562 \\
        & Gemini-2.5-Flash & 0.500 & 0.640 & 0.584 & 0.567 \\
        & Gemini-3-Flash & \textbf{0.525} & \textbf{0.660} & \textbf{0.606} & \textbf{0.591} \\
        \hline
        \multirow{2}{*}{D-SG}
        & Qwen3-30B-A3B (no think) & 0.401 & 0.451 & 0.432 & 0.441 \\
        & Qwen3-30B-A3B (think) & \textbf{0.408} & \textbf{0.493} & \textbf{0.468} & \textbf{0.456} \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Observation: reasoning capability drives accuracy.}
A clear trend emerges from both experiments.
On DHRD-SE, progressing from Gemini-2.0-Flash to Gemini-3-Flash---each generation representing improved reasoning and instruction-following capabilities---yields a monotonic increase: HR@3 improves from $0.606$ to $0.660$ ($+8.9\%$ relative) and NDCG@3 from $0.561$ to $0.606$ ($+8.0\%$ relative).
On DHRD-SG, enabling thinking mode for Qwen3-30B-A3B boosts HR@3 from $0.451$ to $0.493$ ($+9.3\%$ relative) and NDCG@3 from $0.432$ to $0.468$ ($+8.3\%$ relative).
These improvements are achieved with identical prompts, candidate sets, and collaborative signals---the \emph{only} variable is the model's reasoning capacity.

This observation has a practical implication: since recommendation accuracy scales with reasoning quality, one can potentially achieve strong performance with a small, efficient model \emph{fine-tuned on distilled reasoning traces} from a larger teacher model.
Rather than deploying a 32B-parameter model at inference time, a smaller student model trained to reproduce the chain-of-thought outputs of the teacher could retain much of the reasoning benefit at a fraction of the computational cost.
We leave this distillation approach to future work.

%% ---- Multi-Level Analysis ----

\subsection{Analysis}

\paragraph{Candidate Coverage Bottleneck.}
A key finding is that performance is bounded by whether the ground-truth vendor survives the geohash-based candidate filtering stage.
On DHRD-SE, the ground truth is present in only 72.4\% of candidate sets.
Among samples where the ground truth \emph{is} present, the conditional HR@3 reaches approximately $0.91$, indicating that the Critic's vendor ranking is highly effective once the correct cuisine and delivery area are identified.
This suggests that improving candidate generation---through expanded geohash neighborhoods, multi-cuisine fallbacks, or hybrid retrieval---offers the most direct path to overall accuracy gains.

\paragraph{Round~1 Cuisine Accuracy.}
Round~1 cuisine prediction is substantially more accurate than the final vendor-level metrics suggest.
On DHRD-SG with thinking enabled, the Analyzer achieves a cuisine-level HR@3 of $0.845$, meaning the ground-truth cuisine appears in the top-3 predictions for 84.5\% of samples.
This confirms that the combination of LightGCN scores and order-history reasoning effectively captures cuisine-level preferences, and that the cuisine $\rightarrow$ vendor narrowing is the more challenging step.

\paragraph{Temporal and Behavioral Patterns.}
Dinner-time predictions (18:00--21:00) outperform off-peak windows, consistent with more regular ordering behavior during primary meal times.
Users with long histories and strong vendor loyalty show markedly better accuracy, confirming that the agents exploit habitual ordering cues.
High-frequency cuisines (pizza, Asian, burgers) achieve stronger HR@K thanks to richer training signals, whereas sparse cuisines challenge both LightGCN and Swing.

%% ---- Ablation Study ----

\subsection{Ablation Study}

\paragraph{}
Table~\ref{tab:ablation} reports leave-one-component-out ablations.
Excluding LightGCN scores from Round~1 diminishes cuisine recall, confirming that embeddings provide complementary collaborative evidence beyond sequential order histories.
Omitting similar-user records from Round~2 is the single most harmful vendor-level removal, demonstrating that peer behavior is critical once the candidate set is defined.
Without geohash filtering, the candidate list balloons and overwhelms the Critic, degrading ranking quality.
Collapsing the pipeline to a single round (cuisine prediction only) eliminates vendor-level precision entirely.
Collectively, the ablations confirm that each component---collaborative signals, spatial filtering, and two-stage LLM reasoning---contributes to the final accuracy.

\begin{table}[t]
    \centering
    \caption{Ablation study on DHRD-SE repeat-order evaluation. Replace placeholders with measurements when available.}
    \label{tab:ablation}
    \begin{tabular}{lcccc}
        \hline
        Variant & HR@1 & HR@3 & NDCG@3 & MRR \\
        \hline
        Full Pipeline & -- & -- & -- & -- \\
        \quad w/o LightGCN (Round~1) & -- & -- & -- & -- \\
        \quad w/o Similar Users (Round~2) & -- & -- & -- & -- \\
        \quad w/o Geohash Filtering & -- & -- & -- & -- \\
        \quad w/o Round~2 (cuisine only) & -- & -- & -- & -- \\
        \hline
    \end{tabular}
\end{table}

\subsection{Discussion}




\paragraph{Training-free competitive performance.}
A notable property of our agentic pipeline is that it requires \emph{no task-specific training}.
The only learned components are LightGCN (trained on the user--cuisine bipartite graph with standard BPR loss) and Swing (a parameter-free co-purchase statistic).
All recommendation reasoning is performed by a general-purpose LLM conditioned on structured prompts.
Despite this, our NDCG@3 on DHRD-SE ($0.606$) surpasses DPVP ($0.550$), a dedicated food delivery model trained end-to-end with domain-specific graph architectures and hyperparameter tuning.
This suggests that the combination of lightweight collaborative signals and LLM reasoning can match---and in some metrics exceed---specialized trained models for repeat-order prediction.

\paragraph{Interpretability.}
Unlike neural recommendation models that produce opaque scores, every prediction in our pipeline comes with an explicit reasoning trace: the Analyzer explains why it selected certain cuisines, and the Critic reflects on how it balanced vendor loyalty, temporal patterns, and peer evidence.
This interpretability is valuable both for debugging recommendation failures and for building user trust in production systems.

\paragraph{Scalability via distillation.}
The consistent relationship between model reasoning capability and recommendation accuracy (Section~\ref{sec:model_comparison}) suggests a path toward efficient deployment.
As LLMs with stronger thinking abilities become available, their reasoning traces can be distilled into smaller, specialized models through supervised fine-tuning.
A compact student model trained on the teacher's chain-of-thought outputs could retain most of the reasoning benefit while meeting latency and cost requirements for production food delivery recommendation.



\subsection{Multi-Level Analysis}
\paragraph{}A candidate-coverage study reveals that performance hinges on whether the ground-truth vendor survives geohash filtering; once present, Hit@1 rises sharply, highlighting the importance of accurate cuisine forecasts and spatial indexing upstream. High-frequency cuisines such as pizza, burgers, and Asian fare achieve stronger Hit@K thanks to richer training signals, whereas sparse cuisines challenge both LightGCN and Swing. Temporal effects mirror behavioral regularities: dinner-time predictions (18:00--21:00) outperform off-peak windows, and weekday/weekend differences align with the user-profiler's temporal features. Users with long histories and strong vendor loyalty show markedly better accuracy, confirming that the agents exploit habitual ordering cues, while highly diverse cuisine explorers remain harder to predict.

\subsection{Ablation Study}
\paragraph{}Table~\ref{tab:ablation} reports leave-one-component-out ablations. Excluding LightGCN scores from Round~1 diminishes cuisine recall, proving that embeddings provide complementary collaborative evidence beyond sequential order histories. Omitting similar-user records from Round~2 is the single most harmful vendor-level removal, demonstrating that peer behavior is critical once the candidate set is defined. Without geohash filtering, the candidate list balloons and overwhelms the Critic, while collapsing the pipeline to a single round (cuisine prediction only) eliminates vendor-level precision entirely. Collectively, the ablations confirm that each agentic component---collaborative signals, spatial filtering, and two-stage LLM reasoning---contributes indispensably to the final accuracy.

\begin{table}[t]
    \centering
    \caption{Ablation study on the DHRD-SE repeat-order evaluation. Replace placeholders with actual measurements when available.}
    \label{tab:ablation}
    \begin{tabular}{lcccc}
        \hline
        Variant & Hit@1 & Hit@3 & Hit@5 & MRR \\
        \hline
        Full Pipeline & -- & -- & -- & -- \\
        w/o LightGCN context (R1) & -- & -- & -- & -- \\
        w/o Similar Users (R2) & -- & -- & -- & -- \\
        w/o Geohash Filtering & -- & -- & -- & -- \\
        w/o Round~2 (R1 cuisine only) & -- & -- & -- & -- \\
        \hline
    \end{tabular}
\end{table}

\section{Discussion}

\subsection{Performance on Sparse Signals: ID-based vs. Agentic Models}
Beyond the ablation studies presented in Table~\ref{tab:ablation}, we extended our evaluation to contrast our Agentic framework against strong graph-based baselines, specifically LightGCN, across varying task granularities. We observed a stark divergence in performance when shifting the recommendation target from coarse-grained labels (e.g., \textit{Cuisine}) to fine-grained entities (e.g., \textit{Vendor}).

While LightGCN remained competitive for cuisine classification, its performance degraded significantly for vendor recommendation. We attribute this to the **interaction sparsity** inherent in the vendor space. The cuisine manifold is relatively low-dimensional and densely connected; thus, the user-item interaction graph contains sufficient edge density for message-passing mechanisms to learn representative embeddings. In contrast, the vendor space is high-dimensional with a long-tail distribution, resulting in a sparse graph where ID-based collaborative signals become weak or disconnected.

This comparison underscores the core advantage of our Agentic solution. Unlike traditional collaborative filtering which relies on dense historical priors, our agent leverages **semantic reasoning** and **general world knowledge**. By treating recommendation as a reasoning problem rather than a signal-matching problem, the agent effectively mitigates data scarcity, maintaining high performance even in scenarios where collaborative signals are insufficient.

\subsection{Inference Time Scaling and Reasoning Capabilities}
To isolate the impact of model capacity and reasoning depth, we evaluated our framework across a spectrum of frontier models, encompassing both open-source and proprietary architectures. Our experiments reveal two pivotal scaling laws in the context of recommendation:

\begin{itemize}
    \item \textbf{Model Scale Correlation:} Consistent with general NLP trends, larger models demonstrated superior performance, correlating with their broader knowledge base and improved instruction-following capabilities.
    \item \textbf{The Necessity of "System 2" Thinking:} Crucially, we found that enabling "thinking" steps (i.e., inference-time reasoning) yielded substantial performance gains independent of model size. 
\end{itemize}

These findings suggest that complex recommendation tasks require more than intuitive pattern matching ("System 1"). By allocating more compute at inference time, the agentic workflow mimics "System 2" processing allowing the model to decompose complex user intents, weigh conflicting constraints (e.g., delivery time vs. rating), and perform **self-correction** on its initial ranking candidates. This demonstrates that **inference-time scaling** is not solely an NLP phenomenon but a critical lever for next-generation recommender systems, enabling them to handle the nuance of user preference with greater precision than single-pass generation.

\subsection{The Diminishing Returns of Supervised Fine-Tuning}
Finally, we investigated the efficacy of Supervised Fine-Tuning (SFT) as a mechanism for performance enhancement. Contrary to the prevailing paradigm in earlier LLM-based recommendation works, our results indicate that SFT yields only marginal improvements compared to the zero-shot Agentic workflow.

When weighing the computational cost and engineering overhead of SFT against the performance delta, the Return on Investment (ROI) is suboptimal. We hypothesize that while SFT improves the model's adherence to output formatting, it contributes little to—and potentially degrades—the model's complex reasoning capabilities due to the risk of catastrophic forgetting or overfitting to limited training templates. This observation aligns with recent industry findings, such as those by Wang et al.~\cite{wang2025reaseq}, which suggest that reasoning capabilities are better elicited through prompt engineering and inference scaling than through parameter updates.

Consequently, we argue that the frontier of recommender system optimization lies not in blindly optimizing model parameters, but in the **architectural design of the Agentic flow**. Empowering the model with the right tools, memory mechanisms, and planning algorithms offers a more scalable and effective path to improvement than traditional fine-tuning.

\section{Conclusion}
In this work, we introduced a novel Agentic Recommender System framework that fundamentally shifts the ranking paradigm from pattern matching to reasoning-driven decision-making. By harnessing the advanced reasoning capabilities of LLMs, our approach effectively bridges the semantic gap that limits traditional ID-based methods.

Our empirical results demonstrate that this agentic workflow outperforms state-of-the-art graph-based model like DPVP. Notably, our framework achieves these results with high data efficiency, requiring only the minimal training of a lightGCN head rather than the extensive hyperparameter tuning and large-scale retraining inherent to graph neural networks.

Furthermore, this study establishes a critical precedent for the role of inference-time scaling in recommendation systems. We provide compelling evidence that allocating compute to "deliberative reasoning"—allowing the model to think and self-correct at test time—yields significant gains in ranking accuracy. This finding suggests that future improvements in recommender systems may stem as much from test-time computation as from model scaling.

Finally, we position this work not merely as a standalone model, but as a foundational, modular architecture. By decoupling the reasoning engine from the retrieval and tool-use components, our framework allows for the seamless integration of future frontier models to further enhance performance. We believe this agentic approach represents a vital step toward the next generation of recommender systems—ones that merge vast world knowledge with intricate logical reasoning to deliver truly intent-aware recommendations.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
