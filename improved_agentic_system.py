#!/usr/bin/env python3
"""
Improved Agentic Recommendation System
Incorporates fixes based on analysis report findings.
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import random

# Add project to path
sys.path.insert(0, str(Path(__file__).parent))

from agentic_recommender.models.llm_provider import GeminiProvider, DEFAULT_GEMINI_KEY
from agentic_recommender.agents import Manager, Analyst
from agentic_recommender.utils.logging import get_logger


class ImprovedManager(Manager):
    """
    Enhanced Manager with better action selection and multi-step reasoning.
    
    Improvements:
    1. Confidence-based action selection
    2. Enhanced context passing
    3. Multi-step reasoning capability
    """
    
    def __init__(self, thought_llm, action_llm, config: Dict[str, Any] = None):
        super().__init__(thought_llm, action_llm, config)
        self.confidence_threshold = config.get('confidence_threshold', 0.7) if config else 0.7
        self.max_reasoning_steps = config.get('max_reasoning_steps', 3) if config else 3
    
    def should_finish_immediately(self, context: Dict[str, Any]) -> bool:
        """Check if we have enough information to make recommendation immediately"""
        has_sequence = len(context.get('prompt_items', [])) >= 3
        has_names = bool(context.get('item_names', {}))
        has_candidates = len(context.get('candidates', [])) > 0
        has_clear_pattern = self._detect_sequence_pattern(context.get('prompt_items', []))
        
        return has_sequence and has_names and has_candidates and has_clear_pattern
    
    def _detect_sequence_pattern(self, items: List[str]) -> bool:
        """Detect if there's a clear pattern in the item sequence"""
        if len(items) < 2:
            return False
        
        # Simple heuristic: if we have item names, check for category consistency
        # This could be expanded with more sophisticated pattern detection
        return len(items) >= 3  # For now, consider 3+ items as sufficient pattern
    
    def _build_enhanced_action_prompt(self, task_context: Dict[str, Any]) -> str:
        """Build improved action prompt with confidence-based decision making"""
        
        # Check if we should finish immediately
        should_finish = self.should_finish_immediately(task_context)
        
        prompt = f"""Based on your thinking, choose the most appropriate action:

DECISION CRITERIA (High Priority):
- If you have 3+ sequential items AND item names are available ‚Üí Finish[specific_recommendation]
- If clear patterns emerge from user sequence ‚Üí Finish[recommendation] 
- Only use Analyse/Search if critical information is truly missing

CONFIDENCE ASSESSMENT:
- Current sequence length: {len(task_context.get('prompt_items', []))}
- Item names available: {'Yes' if task_context.get('item_names') else 'No'}
- Candidates available: {len(task_context.get('candidates', []))} options
- Should finish immediately: {'Yes' if should_finish else 'No'}

AVAILABLE ACTIONS:
- Analyse[user, user_id] - analyze user preferences (use sparingly)
- Analyse[item, item_id] - analyze specific item (use sparingly)
- Search[query] - search for external information (use sparingly)
- Finish[item_recommendation] - return final recommendation (preferred when sufficient info available)

CURRENT CONTEXT:
{json.dumps(task_context, indent=2)}

SCRATCHPAD:
{self.scratchpad}

Choose the MOST EFFICIENT action. If you have sufficient information for a recommendation, use Finish[item] directly.
Return your action in the exact format shown above."""
        
        return prompt
    
    def _build_action_prompt(self, task_context: Dict[str, Any]) -> str:
        """Override with enhanced action prompt"""
        return self._build_enhanced_action_prompt(task_context)
    
    def recommend_with_reasoning(self, task_context: Dict[str, Any], 
                                max_steps: Optional[int] = None) -> Dict[str, Any]:
        """
        Enhanced recommendation with multi-step reasoning.
        
        Args:
            task_context: Full recommendation context
            max_steps: Maximum reasoning steps (uses default if None)
            
        Returns:
            Dictionary with recommendation and reasoning trace
        """
        max_steps = max_steps or self.max_reasoning_steps
        reasoning_trace = []
        
        # Enhance context with metadata
        enhanced_context = self._prepare_enhanced_context(task_context)
        
        for step in range(max_steps):
            step_info = {'step': step + 1}
            
            # Think phase
            thought = self.think(enhanced_context)
            step_info['thought'] = thought
            
            # Act phase
            action_type, argument = self.act(enhanced_context)
            step_info['action'] = f"{action_type}[{argument}]"
            
            # Check if we're done
            if action_type == "Finish":
                step_info['result'] = argument
                reasoning_trace.append(step_info)
                
                return {
                    'recommendation': argument,
                    'reasoning_trace': reasoning_trace,
                    'steps_taken': step + 1,
                    'final_confidence': 'high'
                }
            
            # Execute action and update context
            if action_type == "Analyse":
                # In a real system, we'd call the actual analyst
                analysis_result = f"Analysis completed for {argument}"
                enhanced_context['analysis'] = analysis_result
                step_info['analysis_result'] = analysis_result
            elif action_type == "Search":
                # In a real system, we'd perform actual search
                search_result = f"Search completed for {argument}"
                enhanced_context['search_result'] = search_result
                step_info['search_result'] = search_result
            
            reasoning_trace.append(step_info)
        
        # Fallback if no decision reached
        fallback_rec = self._generate_fallback_recommendation(enhanced_context)
        
        return {
            'recommendation': fallback_rec,
            'reasoning_trace': reasoning_trace,
            'steps_taken': max_steps,
            'final_confidence': 'low_fallback'
        }
    
    def _prepare_enhanced_context(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare enhanced context with additional metadata"""
        enhanced = task_context.copy()
        
        # Add sequence analysis
        prompt_items = task_context.get('prompt_items', [])
        if prompt_items:
            enhanced['sequence_length'] = len(prompt_items)
            enhanced['sequence_patterns'] = self._analyze_sequence_patterns(prompt_items)
        
        # Add item metadata if available
        item_names = task_context.get('item_names', {})
        if item_names:
            enhanced['category_info'] = self._infer_categories(item_names)
            enhanced['item_diversity'] = len(set(item_names.values())) / len(item_names) if item_names else 0
        
        # Add recommendation context
        candidates = task_context.get('candidates', [])
        if candidates:
            enhanced['num_candidates'] = len(candidates)
            enhanced['recommendation_space'] = 'focused' if len(candidates) <= 10 else 'broad'
        
        return enhanced
    
    def _analyze_sequence_patterns(self, items: List[str]) -> Dict[str, Any]:
        """Analyze patterns in item sequence"""
        return {
            'length': len(items),
            'has_repetition': len(items) != len(set(items)),
            'progression': 'sequential' if len(items) >= 3 else 'short'
        }
    
    def _infer_categories(self, item_names: Dict[str, str]) -> Dict[str, Any]:
        """Infer categories from item names"""
        categories = {
            'beauty': ['foundation', 'mascara', 'lipstick', 'concealer', 'powder', 'serum'],
            'food': ['pizza', 'burger', 'salad', 'drink', 'coffee', 'sandwich'],
            'tech': ['laptop', 'keyboard', 'monitor', 'mouse', 'headset', 'webcam'],
            'books': ['book', 'novel', 'guide', 'manual', 'notebook']
        }
        
        detected_categories = set()
        for name in item_names.values():
            name_lower = name.lower()
            for category, keywords in categories.items():
                if any(keyword in name_lower for keyword in keywords):
                    detected_categories.add(category)
        
        return {
            'detected_categories': list(detected_categories),
            'primary_category': list(detected_categories)[0] if detected_categories else 'general',
            'is_focused': len(detected_categories) <= 1
        }
    
    def _generate_fallback_recommendation(self, context: Dict[str, Any]) -> str:
        """Generate fallback recommendation when reasoning doesn't converge"""
        candidates = context.get('candidates', [])
        
        if candidates:
            # Simple fallback: return first candidate
            return candidates[0]
        else:
            return "no_recommendation_available"


class ImprovedAnalyst(Analyst):
    """
    Enhanced Analyst with better context utilization.
    
    Improvements:
    1. Better metadata integration
    2. Category-aware analysis
    3. Sequential pattern recognition
    """
    
    def _analyze_user_enhanced(self, user_id: str, context: Dict[str, Any], 
                              json_mode: bool = False) -> str:
        """Enhanced user analysis with full context"""
        
        # Extract rich information from context
        prompt_items = context.get('prompt_items', [])
        item_names = context.get('item_names', {})
        category_info = context.get('category_info', {})
        
        # Build comprehensive analysis prompt
        prompt = f"""Analyze this user's sequential behavior for recommendation:

USER: {user_id}

SEQUENTIAL BEHAVIOR:
- Items in sequence: {prompt_items}
- Item names: {item_names}
- Detected categories: {category_info.get('detected_categories', [])}
- Primary category: {category_info.get('primary_category', 'unknown')}

SEQUENCE ANALYSIS:
- Length: {len(prompt_items)} items
- Pattern: {context.get('sequence_patterns', {}).get('progression', 'unknown')}
- Category focus: {'Focused' if category_info.get('is_focused', False) else 'Diverse'}

TASK: Provide insights about:
1. User's clear preference patterns from this sequence
2. Sequential progression (what leads to what)
3. Next logical item in sequence
4. Confidence in recommendation

{"Return analysis in JSON format with keys: patterns, progression, next_item, confidence" if json_mode else "Provide detailed analysis with clear next-item recommendation."}
"""
        
        return self.llm_provider.generate(prompt, temperature=0.7, max_tokens=300)


class ImprovedAgenticSystem:
    """
    Complete improved agentic recommendation system.
    
    Features:
    1. Enhanced Manager with confidence-based decisions
    2. Improved Analyst with rich context
    3. Multi-step reasoning with fallback
    4. Comprehensive logging and metrics
    """
    
    def __init__(self, use_real_gemini: bool = True):
        self.use_real_gemini = use_real_gemini
        self.logger = get_logger()
        
        # Initialize LLM providers
        if use_real_gemini:
            self.llm = GeminiProvider(DEFAULT_GEMINI_KEY)
        else:
            from agentic_recommender.models.llm_provider import MockLLMProvider
            self.llm = MockLLMProvider()
        
        # Initialize improved agents
        config = {
            'confidence_threshold': 0.7,
            'max_reasoning_steps': 3
        }
        
        self.manager = ImprovedManager(self.llm, self.llm, config)
        self.analyst = ImprovedAnalyst(self.llm)
        
        print(f"üöÄ Improved Agentic System initialized")
        print(f"   Using real Gemini: {use_real_gemini}")
    
    def recommend(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate recommendation for a sample.
        
        Args:
            sample: Evaluation sample with prompt_items, candidates, etc.
            
        Returns:
            Complete recommendation result with reasoning
        """
        # Prepare task context
        task_context = {
            'user_id': sample['user_id'],
            'prompt_items': sample['prompt_items'],
            'candidates': sample.get('candidates', []),
            'item_names': sample.get('item_names', {}),
            'target_item': sample.get('target_item'),
            'session_id': sample.get('session_id')
        }
        
        # Use enhanced reasoning
        result = self.manager.recommend_with_reasoning(task_context)
        
        # Add evaluation metrics
        if 'target_item' in sample:
            result['target_item'] = sample['target_item']
            result['correct_prediction'] = result['recommendation'] == sample['target_item']
            
            # Check if target is in candidates
            if sample.get('candidates') and sample['target_item'] in sample['candidates']:
                target_rank = sample['candidates'].index(sample['target_item']) + 1
                result['target_rank'] = target_rank
        
        return result
    
    def batch_recommend(self, samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate recommendations for multiple samples"""
        results = []
        
        for i, sample in enumerate(samples):
            print(f"Processing sample {i+1}/{len(samples)}: {sample.get('session_id', 'unknown')}")
            
            try:
                result = self.recommend(sample)
                results.append(result)
                
                # Print summary
                rec = result['recommendation']
                steps = result['steps_taken']
                confidence = result['final_confidence']
                correct = '‚úÖ' if result.get('correct_prediction', False) else '‚ùå'
                
                print(f"  Result: {rec} ({steps} steps, {confidence}) {correct}")
                
            except Exception as e:
                print(f"  Error: {e}")
                results.append({
                    'recommendation': 'error',
                    'error': str(e),
                    'sample_id': sample.get('session_id', 'unknown')
                })
        
        return results
    
    def evaluate_performance(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Evaluate system performance on batch results"""
        valid_results = [r for r in results if 'error' not in r]
        
        if not valid_results:
            return {'error': 'No valid results to evaluate'}
        
        # Calculate metrics
        correct_predictions = sum(1 for r in valid_results if r.get('correct_prediction', False))
        total_predictions = len(valid_results)
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        # Average steps and confidence
        avg_steps = sum(r.get('steps_taken', 0) for r in valid_results) / len(valid_results)
        high_confidence = sum(1 for r in valid_results if r.get('final_confidence') == 'high')
        confidence_rate = high_confidence / len(valid_results)
        
        # API performance
        model_info = self.llm.get_model_info() if hasattr(self.llm, 'get_model_info') else {}
        
        return {
            'total_samples': len(results),
            'valid_results': len(valid_results),
            'accuracy': accuracy,
            'correct_predictions': correct_predictions,
            'avg_steps_per_recommendation': avg_steps,
            'high_confidence_rate': confidence_rate,
            'api_calls': model_info.get('total_calls', 0),
            'avg_api_time': model_info.get('avg_time_per_call', 0),
            'errors': len(results) - len(valid_results)
        }


def main():
    """Demonstrate improved agentic system"""
    print("üß™ Testing Improved Agentic Recommendation System")
    print("="*60)
    
    # Initialize system
    system = ImprovedAgenticSystem(use_real_gemini=True)
    
    # Create test samples
    test_samples = [
        {
            "session_id": "improved_demo_1",
            "user_id": "tech_user",
            "prompt_items": ["gaming_laptop", "mechanical_keyboard", "gaming_mouse"],
            "target_item": "gaming_monitor",
            "candidates": ["gaming_monitor", "mouse_pad", "webcam", "headset", "speakers"],
            "item_names": {
                "gaming_laptop": "ASUS ROG Gaming Laptop",
                "mechanical_keyboard": "Corsair RGB Mechanical Keyboard",
                "gaming_mouse": "Logitech G Pro Gaming Mouse",
                "gaming_monitor": "ASUS 27\" 4K Gaming Monitor"
            }
        },
        {
            "session_id": "improved_demo_2",
            "user_id": "beauty_user",
            "prompt_items": ["foundation", "concealer", "setting_powder"],
            "target_item": "blush",
            "candidates": ["blush", "mascara", "lipstick", "eyeshadow", "bronzer"],
            "item_names": {
                "foundation": "Liquid Foundation SPF 30",
                "concealer": "Under-Eye Concealer",
                "setting_powder": "Translucent Setting Powder",
                "blush": "Natural Pink Blush"
            }
        }
    ]
    
    # Run recommendations
    print(f"\nüöÄ Running {len(test_samples)} improved recommendations...")
    results = system.batch_recommend(test_samples)
    
    # Evaluate performance
    print(f"\nüìä Performance Evaluation:")
    performance = system.evaluate_performance(results)
    
    for key, value in performance.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")
    
    # Print detailed results
    print(f"\nüìã Detailed Results:")
    for i, (sample, result) in enumerate(zip(test_samples, results)):
        print(f"\nSample {i+1}: {sample['session_id']}")
        print(f"   Sequence: {' ‚Üí '.join(sample['prompt_items'])}")
        print(f"   Target: {sample['target_item']}")
        print(f"   Recommendation: {result.get('recommendation', 'error')}")
        print(f"   Correct: {'‚úÖ' if result.get('correct_prediction', False) else '‚ùå'}")
        print(f"   Steps: {result.get('steps_taken', 0)}")
        print(f"   Confidence: {result.get('final_confidence', 'unknown')}")
        
        if 'reasoning_trace' in result:
            print(f"   Reasoning trace:")
            for step_info in result['reasoning_trace']:
                print(f"     Step {step_info['step']}: {step_info.get('action', 'N/A')}")
    
    print(f"\n‚úÖ Improved system demonstration completed!")


if __name__ == "__main__":
    main()