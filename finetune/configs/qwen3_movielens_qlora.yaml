# Base LLaMA-Factory config for Qwen3-8B MovieLens LoRA (rank=16, 4-bit).
#
# Usage:
#   python scripts/train_llamafactory.py --config configs/qwen3_movielens_qlora.yaml

stage: sft
do_train: true
model_name_or_path: Qwen/Qwen3-8B-Instruct
template: qwen
finetuning_type: lora
dataset_dir: data
dataset: movielens_qwen3
cutoff_len: 2048
max_samples: null

output_dir: output/qwen3-movielens-qlora
logging_steps: 10
save_steps: 200
eval_steps: 200
save_total_limit: 2

per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.5e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.03
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
max_grad_norm: 1.0

quantization_bit: 4
quantization_method: bnb
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16

lora_target: all
lora_rank: 16
lora_alpha: 64
lora_dropout: 0.05
use_rslora: false
add_lora_modules: null

gradient_checkpointing: true
ddp_timeout: 180000000
torch_dtype: bfloat16
flash_attn: auto

val_size: 0.0
evaluation_strategy: steps
fp16: false
bf16: true

report_to:
  - tensorboard
