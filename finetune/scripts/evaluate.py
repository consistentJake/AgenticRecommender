"""Before/after evaluator for Qwen3 MovieLens recommendation fine-tuning."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

try:
    from transformers import BitsAndBytesConfig
except ImportError:  # pragma: no cover - bitsandbytes optional on CPU
    BitsAndBytesConfig = None  # type: ignore

try:
    from peft import PeftModel
except ImportError as exc:  # pragma: no cover
    raise SystemExit(
        "peft is required for evaluation. Run `pip install peft`."
    ) from exc


SYSTEM_PROMPT = (
    "You are a movie recommendation assistant. Given a user's recent history "
    "and a candidate movie, respond with exactly one word: Yes or No."
)
USER_TEMPLATE = (
    "User's last {n} watched movies:\n{history}\n\n"
    "Candidate movie:\n{candidate}\n\n"
    "Should we recommend this movie to the user? Answer Yes or No."
)
MAX_NEW_TOKENS = 4


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--test-file",
        type=Path,
        required=True,
        help="Path to test_raw.jsonl generated by prepare_movielens.py",
    )
    parser.add_argument(
        "--base-model",
        type=str,
        required=True,
        help="Base model id (e.g., Qwen/Qwen3-8B-Instruct).",
    )
    parser.add_argument(
        "--lora-output",
        type=Path,
        default=None,
        help="Directory containing the trained LoRA adapters.",
    )
    parser.add_argument(
        "--device",
        choices=("auto", "cuda", "cpu"),
        default="auto",
        help="Device preference for inference.",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=500,
        help="Limit evaluation to the first N valid samples.",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=MAX_NEW_TOKENS,
        help="Override max_new_tokens during generation.",
    )
    parser.add_argument(
        "--no-quantize",
        action="store_true",
        help="Disable 4-bit loading even if CUDA is available.",
    )
    return parser.parse_args()


def load_jsonl(path: Path) -> List[Dict]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


def resolve_device(choice: str) -> str:
    if choice == "cpu":
        return "cpu"
    if choice == "cuda":
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA requested but no GPU detected.")
        return "cuda"
    # auto
    return "cuda" if torch.cuda.is_available() else "cpu"


def should_quantize(device: str, disabled: bool) -> bool:
    return (
        device == "cuda"
        and not disabled
        and BitsAndBytesConfig is not None
    )


def load_model_and_tokenizer(
    model_id: str,
    device: str,
    quantize: bool,
    adapter_dir: Path | None,
) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    model_kwargs: Dict = {"trust_remote_code": True}
    compute_dtype = torch.bfloat16 if device == "cuda" else torch.float32
    if quantize:
        model_kwargs["device_map"] = "auto"
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=compute_dtype,
        )
    else:
        model_kwargs["torch_dtype"] = compute_dtype
        model_kwargs["device_map"] = "auto" if device == "cuda" else {"": "cpu"}

    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
    if adapter_dir:
        model = PeftModel.from_pretrained(model, adapter_dir)
    model.eval()
    return model, tokenizer


def build_messages(row: Dict) -> List[Dict[str, str]]:
    history_block = "\n".join(row["history_titles"])
    user_content = USER_TEMPLATE.format(
        n=len(row["history_titles"]),
        history=history_block,
        candidate=row["candidate_title"],
    )
    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]


def normalize_prediction(text: str) -> str | None:
    text = text.strip().lower()
    if text.startswith("yes"):
        return "Yes"
    if text.startswith("no"):
        return "No"
    return None


def evaluate(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    rows: List[Dict],
    max_samples: int,
    max_new_tokens: int,
) -> Tuple[float, int]:
    total = 0
    correct = 0
    device = next(model.parameters()).device
    for row in rows:
        if total >= max_samples:
            break
        label = row["label"]
        messages = build_messages(row)
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.pad_token_id,
                do_sample=False,
            )
        gen_text = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1] :],
            skip_special_tokens=True,
        )
        pred = normalize_prediction(gen_text)
        if pred is None:
            continue
        total += 1
        if pred == label:
            correct += 1
    acc = correct / total if total else 0.0
    return acc, total


def main() -> None:
    args = parse_args()
    device = resolve_device(args.device)
    rows = load_jsonl(args.test_file)
    quantize = should_quantize(device, args.no_quantize)

    print(f"[eval] Loading base model on {device} (quantize={quantize})")
    base_model, base_tokenizer = load_model_and_tokenizer(
        args.base_model, device, quantize, adapter_dir=None
    )
    base_acc, base_total = evaluate(
        base_model,
        base_tokenizer,
        rows,
        args.max_samples,
        args.max_new_tokens,
    )
    print(f"Base model accuracy: {base_acc:.3f} on {base_total} samples")

    if args.lora_output:
        print(f"[eval] Loading LoRA adapters from {args.lora_output}")
        ft_model, ft_tokenizer = load_model_and_tokenizer(
            args.base_model, device, quantize, adapter_dir=args.lora_output
        )
        ft_acc, ft_total = evaluate(
            ft_model,
            ft_tokenizer,
            rows,
            args.max_samples,
            args.max_new_tokens,
        )
        print(f"Finetuned model accuracy: {ft_acc:.3f} on {ft_total} samples")


if __name__ == "__main__":
    main()
